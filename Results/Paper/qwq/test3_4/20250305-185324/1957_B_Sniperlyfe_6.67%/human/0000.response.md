Alright, I've got this problem to solve. Let's see what it's asking for. So, I'm given two integers, n and k, and I need to construct a sequence of n non-negative integers that sum up to k. Additionally, I need to maximize the number of 1s in the binary representation of the bitwise OR of these numbers.

First, I need to understand what bitwise OR does. Bitwise OR takes two numbers and performs the OR operation on each pair of corresponding bits. For example, 1 OR 2 is 3 because in binary, 1 is 01 and 2 is 10, and OR'ing them gives 11, which is 3.

So, the goal is to have the bitwise OR of all these n numbers have as many 1s in its binary representation as possible.

Let me think about how to approach this.

I need to maximize the number of 1s in the binary representation of the OR of all the numbers in the sequence. That means I want as many bits set to 1 in the OR result as possible.

Given that OR combines the bits, a bit in the OR result is 1 if at least one of the numbers in the sequence has that bit set to 1.

So, to maximize the number of 1s in the OR result, I need to have as many different bits set to 1 across all the numbers in the sequence as possible.

Wait, but there's a constraint: the sum of the numbers must be k.

So, I need to distribute k among n numbers such that their sum is k, and the OR of these numbers has as many 1s in its binary representation as possible.

Let me consider some examples to get a better understanding.

Take the second test case from the example:

n = 2, k = 3

Possible sequences:

- [1, 2]: sum is 3, and 1 | 2 = 3 (binary 11), which has two 1s.

- [0, 3]: sum is 3, and 0 | 3 = 3 (binary 11), same as above.

- [3, 0]: same as above.

- [1, 2]: same as above.

So, in this case, the maximum number of 1s in the OR is 2.

Another example:

n = 1, k = 5

Only one number, which is 5. Its binary is 101, which has two 1s.

Another example from the problem:

n = 2, k = 5

Possible sequences:

- [5, 0]: 5 | 0 = 5 (binary 101, two 1s)

- [1, 4]: 1 | 4 = 5 (binary 101, two 1s)

- [3, 2]: 3 | 2 = 3 (binary 11, two 1s)

- [4, 1]: 4 | 1 = 5 (binary 101, two 1s)

So, in this case, the maximum is still two 1s.

Wait, but the problem shows an output of [5,0], which has two 1s in the OR.

But in the fourth test case:

n = 6, k = 51

Output: [3,1,1,32,2,12]

Let's compute the OR of these numbers:

3 is 00000011

1 is 00000001

1 is 00000001

32 is 00100000

2 is 00000010

12 is 00001100

OR'ing them together:

Starting with 00000000

OR 00000011: 00000011

OR 00000001: 00000011

OR 00000001: still 00000011

OR 00100000: 00100011

OR 00000010: 00100011

OR 00001100: 00101111

So, the OR result is 00101111, which is 47 in decimal, and it has five 1s in its binary representation.

So, the goal is to have the OR of all these numbers have as many 1s in its binary as possible.

Now, to maximize the number of 1s in the OR, I need to set as many bits to 1 in the OR as possible, which means covering as many bit positions with at least one number having that bit set to 1.

Given that, I need to distribute the sum k among n numbers such that as many bit positions as possible are set to 1 in at least one of the numbers.

One way to approach this is to assign numbers that cover different bit positions.

For example, if I have n numbers, I can assign numbers that have only one bit set, and each number has a different bit set, up to the number of numbers I have.

But there's a constraint that the sum must be k.

So, I need to assign numbers such that their sum is k, and as many different bits as possible are set to 1 in the OR.

Wait, but there's a trade-off here.

If I set each number to have a different bit set to 1, then the OR will have those bits set to 1.

But I also need to make sure that the sum of these numbers is k.

So, I need to select a set of numbers where each number has at least one unique bit set to 1, and their sum is k.

But, if I have more numbers than the number of bits set in k, things might get tricky.

Wait, maybe another approach.

Let me consider the binary representation of k.

Suppose k is represented as b_m b_{m-1} ... b_0 in binary.

To maximize the number of 1s in the OR, I need to have as many bits from b_m to b_0 set to 1 in at least one of the numbers in the sequence.

Given that, I can try to distribute the set bits of k among the n numbers such that each number gets some of these set bits, and possibly some combinations.

But this seems a bit complicated.

Let me think differently.

If I have n numbers, and I need their OR to have as many 1s as possible, I should try to set different bits in different numbers.

But I also need their sum to be k.

So, perhaps I can start by assigning the smallest possible numbers to n-1 numbers, and assign the remainder to the last number.

Wait, but that might not maximize the number of 1s in the OR.

Wait, in the second test case, n=2, k=3.

If I assign 1 and 2, their OR is 3, which has two 1s in binary.

If I assign 0 and 3, their OR is 3, same as above.

So, in both cases, the OR has two 1s.

In the first test case, n=1, k=5, the OR is 5, which is 101 in binary, two 1s.

In the third test case, n=2, k=5, assigning 5 and 0 gives OR 5, which is 101, two 1s.

Another possible assignment is 3 and 2, OR is 3 (11 in binary), which also has two 1s.

Wait, but in the fourth test case, n=6, k=51, the output is [3,1,1,32,2,12], and their OR is 47 (101111 in binary), which has five 1s.

But 51 in binary is 110011, which has four 1s.

So, somehow, by distributing k among multiple numbers, we can achieve an OR with more 1s than the binary representation of k has.

Wait, but how is that possible?

Wait, 51 in binary is 110011, which is 1s in positions 0,1,4,5 (0-indexed from the right).

The OR of [3,1,1,32,2,12] is 3 | 1 | 1 | 32 | 2 | 12 = 3 (0011) | 1 (0001) | 1 (0001) | 32 (100000) | 2 (0010) | 12 (1100) = 101111 (47 in decimal), which has 1s in positions 0,1,2,3,5.

Wait, but k=51 is 110011 (positions 0,1,4,5), and the OR has positions 0,1,2,3,5 set to 1.

So, by distributing k among multiple numbers, we can set additional bits in the OR that aren't set in k itself.

But, of course, we can't set bits that aren't in k, because the sum is k.

Wait, but in this case, the OR can have more 1s than k, but the sum still needs to be k.

Wait, in the fourth test case, sum is 3+1+1+32+2+12 = 51, and OR is 47, which has five 1s, while 51 has four 1s.

So, by distributing k among multiple numbers, we can set more bits in the OR, as long as the sum remains k.

So, perhaps the strategy is to assign numbers with specific bits set to cover as many bit positions as possible, while ensuring their sum is k.

This seems a bit tricky.

Let me think about the general approach.

I need to maximize the number of 1s in the OR of n numbers that sum to k.

To maximize the number of 1s in the OR, I need to have as many bits set to 1 in the OR as possible.

Each bit in the OR is set if at least one of the numbers has that bit set.

So, for each bit position, I need to ensure that at least one of the numbers has that bit set to 1.

But, I have n numbers to assign, and their sum must be k.

So, perhaps I can start by assigning the smallest possible numbers to n-1 numbers, and assign the remainder to the last number.

Wait, but that might not maximize the number of 1s in the OR.

Wait, in the second test case, n=2, k=3.

Assigning 1 and 2 gives OR 3, which has two 1s.

Assigning 0 and 3 also gives OR 3, same number of 1s.

So, in this case, it doesn't matter.

But in the fourth test case, n=6, k=51, the OR is 47, which has five 1s, while 51 has four 1s.

So, by distributing k among multiple numbers, I can set more bits in the OR.

Wait, but how?

Because when I OR numbers, a bit in the OR is set if at least one number has that bit set.

So, to set a bit in the OR, I just need one number to have that bit set.

But, if I have multiple numbers with that bit set, it's still only set once in the OR.

So, to maximize the number of 1s in the OR, I need to set as many different bits as possible in at least one of the numbers.

Given that, perhaps the strategy is to assign numbers that cover as many unique bits as possible.

But I also need their sum to be k.

So, maybe I can start by assigning numbers that have single bits set, up to the number of n, and then adjust the last number to make the sum k.

Wait, but if n is larger than the number of bits set in k, that might not work.

Wait, perhaps another way.

Let me consider the binary representation of k.

Suppose k has certain bits set.

To maximize the number of 1s in the OR, I need to set as many bits as possible in at least one of the numbers.

So, I should try to set different bits in different numbers, as much as possible.

But, I need to ensure that the sum is k.

So, perhaps I can distribute the set bits of k among the n numbers, each getting some of the set bits, and possibly some numbers getting multiple bits.

But, I need to make sure that no two numbers have the same set bits, to maximize the number of unique bits in the OR.

Wait, but that's not necessarily true.

Actually, to maximize the number of 1s in the OR, I just need to have as many bits set in at least one of the numbers.

So, if I have n numbers, and I can set different bits in different numbers, that would be ideal.

But, k might not allow that, depending on its value.

Wait, maybe I can think in terms of the binary representation.

Let's consider that each number can contribute some bits to the OR.

To maximize the number of 1s in the OR, I need to have as many bits set to 1 in at least one of the numbers.

So, perhaps I can assign numbers that have only one bit set, and each bit is assigned to a different number, up to the number of set bits in k.

But, if n is larger than the number of set bits in k, then I can still only set those bits.

Wait, but in the fourth test case, they set more bits in the OR than are set in k.

How is that possible?

Wait, let's look back at the fourth test case.

n=6, k=51

Output: [3,1,1,32,2,12]

Sum: 3+1+1+32+2+12 = 51

OR: 3 |1 |1 |32 |2 |12 = 47 (binary 101111), which has five 1s.

But k=51 is binary 110011, which has four 1s.

So, how did we get five 1s in the OR?

Looking at the binary representations:

3: 00000011

1: 00000001

1: 00000001

32: 00100000

2: 00000010

12: 00001100

OR'ing them together:

Position 0: at least one 1 → 1

Position 1: at least one 1 → 1

Position 2: 12 has it set → 1

Position 3: 12 has it set → 1

Position 4: 32 has it set → 1

Position 5: 3 has it set → 1

Wait, position 5: 3 is 00000011, which has position 1 and 0 set, not position 5.

Wait, position 5 is the 6th bit from the right, which is 32 (position 5 in binary).

Wait, 32 is 00100000, which is position 5.

3 is 00000011, positions 0 and 1.

1 is 00000001, position 0.

1 is 00000001, position 0.

2 is 00000010, position 1.

12 is 00001100, positions 2 and 3.

So, OR'ing them together:

Position 0: 1

Position 1: 1

Position 2: 1

Position 3: 1

Position 4: 0

Position 5: 1

So, binary 101111, which is 47, has 1s in positions 0,1,2,3,5.

But k=51 is 110011, which is positions 0,1,4,5.

So, how did position 2 and 3 get set in the OR?

Because 12 has positions 2 and 3 set.

But in k=51, position 2 is not set.

Wait, but the sum is still 51.

How is that possible?

Wait, maybe I'm misunderstanding something.

Wait, 3 +1 +1 +32 +2 +12 = 51.

And OR is 3 |1 |1 |32 |2 |12 = 47 (101111 in binary).

But, isn't k=51, which is 110011 in binary.

So, in the OR, we have positions 0,1,2,3,5 set, which are five 1s.

But in k=51, positions 0,1,4,5 are set, which are four 1s.

So, by distributing k among multiple numbers, we can set additional bits in the OR beyond those set in k.

But, how does that work?

Wait, because when we OR numbers, a bit in the OR is set if at least one number has that bit set.

So, even if that bit isn't set in k, as long as one number has it set, it's set in the OR.

But, doesn't that mean we can set any bit we want, as long as we can afford it in the sum?

Wait, but there's a constraint on the sum being k.

So, perhaps I can set bits in the numbers as long as their sum doesn't exceed k.

Wait, but in this case, they set bits that aren't in k, as long as the sum is k.

Wait, for example, in the sequence [3,1,1,32,2,12], the sum is 51, and the OR is 47.

But 47 is less than 51, and 47 in binary is 101111, while 51 is 110011.

So, how does this work?

Wait, perhaps I need to think differently.

Maybe the key is to maximize the number of 1s in the OR, regardless of the bits set in k, as long as the sum is k.

Wait, but in the problem statement, it says "the number of 1s in the binary representation of a1 | a2 | … | an is maximized".

So, it's about maximizing the number of 1s in the OR, not about the bits set in k.

But, the sum must be k.

So, perhaps I need to assign numbers such that their OR has as many 1s as possible, while their sum is k.

In that case, I need to assign numbers that cover as many bit positions as possible, and adjust their values so that the sum is k.

This seems a bit tricky.

Let me consider an approach.

One way is to assign the smallest possible numbers to n-1 numbers, and assign the remainder to the last number.

For example, assign 1 to n-1 numbers, and k - (n-1) to the last number.

But, this might not maximize the number of 1s in the OR.

Wait, for example, in n=2, k=3.

Assign 1 and 2: OR is 3, which has two 1s.

Assign 1 and 1: OR is 1, which has one 1.

So, assigning 1 and 2 is better.

Similarly, in n=6, k=51, assigning [3,1,1,32,2,12] gives OR 47, which has five 1s.

If I assign [1,1,1,1,1,46], OR is 47, which has two 1s.

Wait, 46 is 101110 in binary, which is positions 1,2,3,5.

Combined with five 1s, the OR would be 101111, which is 47, with five 1s.

Wait, but in this case, it's the same as the example.

But, in the example, they assigned [3,1,1,32,2,12], which also gives OR 47.

So, in this case, both assignments give the same OR.

But, perhaps there are better assignments.

Wait, is there a way to get more than five 1s in the OR while keeping the sum as 51?

Let's see.

47 has five 1s.

Is there a way to get six 1s in the OR with sum 51?

Six 1s would be 63 (111111 in binary), which is greater than 51.

But, wait, can I set all six bits to 1 with sum 51?

63 is 111111, which is 63, but 51 is less than 63.

But, if I have numbers that cover more bits, their OR could have more bits set.

Wait, but their sum is still 51.

Is there a way to have their OR have six 1s while their sum is 51?

Let's see.

Suppose I have numbers that set different bits.

For example, assign numbers with single bits set.

For example, assign [1,2,4,8,16,32], their sum is 63, which is more than 51.

So, that's not possible.

Alternatively, assign [1,2,4,8,16,10], sum is 41, which is less than 51.

But, their OR is 16 | 8 | 4 | 2 | 1 | 10 = 16 | 8 | 4 | 2 | 1 | 1010 = 16 | 8 | 4 | 2 | 1 | 1010 = 11111, which is 31, which has five 1s.

Wait, 16 is 10000

8 is 01000

4 is 00100

2 is 00010

1 is 00001

10 is 01010

OR'ing them together: 1 | 2 | 4 | 8 | 10 | 16 = 11111, which is 31, which has five 1s.

But, sum is 1+2+4+8+10+16=3+1+1+32+2+12=51

Wait, but 1+2+4+8+10+16=41, which is less than 51.

Wait, maybe adjust the last number to make the sum 51.

Assign [1,2,4,8,16,10+10]= [1,2,4,8,16,20], sum is 1+2+4+8+16+20=51

OR is 1 | 2 | 4 | 8 | 16 | 20 = 1 | 2 | 4 | 8 | 16 | 10100 = 16 | 8 | 4 | 2 | 1 | 10100 = 11111 in binary, which is 31, which has five 1s.

Same as before.

So, it seems that with sum 51, the maximum number of 1s in the OR is five.

Is there a way to get six 1s?

Six 1s would require the OR to be 63, which is 111111 in binary.

But, to have the OR be 63, I need to have all bits from position 0 to 5 set in at least one of the numbers.

But, if I have numbers that cover all these bits and their sum is 51, is it possible?

Let's try.

Assign [1,2,4,8,16,18], sum is 1+2+4+8+16+18=51

OR is 1 | 2 | 4 | 8 | 16 | 18 = 1 | 2 | 4 | 8 | 16 | 10010 = 16 | 8 | 4 | 2 | 1 | 10010 = 11111 in binary, which is still 31, five 1s.

Same as before.

Wait, 18 in binary is 10010, which is positions 1 and 4.

Combined with the others, the OR is still 31 (11111), five 1s.

So, seems like I can't get six 1s in the OR with sum 51.

Hence, the maximum is five 1s.

So, in this case, the output is correct.

Now, back to the general approach.

I need to maximize the number of 1s in the OR of n numbers that sum to k.

To maximize the number of 1s in the OR, I need to set as many bits as possible to 1 in at least one of the numbers.

Given that, I should try to assign numbers such that each number sets a unique set of bits, and together they cover as many bits as possible.

But, I need to ensure that their sum is k.

One way to approach this is to assign the smallest possible numbers to n-1 numbers, and assign the remainder to the last number.

For example, assign 1 to n-1 numbers, and k - (n-1) to the last number.

In this way, the OR will be 1 | 1 | ... | (k - n + 1).

But, this might not maximize the number of 1s in the OR.

Wait, in the second test case, n=2, k=3.

Assign 1 and 2: OR is 3, which has two 1s.

Assign 1 and (3-1)=2: same as above.

So, it's the same.

In the fourth test case, n=6, k=51.

Assign 1 to five numbers, and 51-5=46 to the last number.

OR is 1 | 1 | 1 | 1 | 1 | 46.

46 is 101110 in binary.

So, OR is 1 | 1 | 1 | 1 | 1 | 46 = 47 (101111 in binary), which has five 1s.

Same as the example output.

So, in this case, it's the same.

But, perhaps there's a better way.

Wait, in the fourth test case, the example output is [3,1,1,32,2,12], which also gives OR 47, five 1s.

So, it's the same as assigning [1,1,1,1,1,46].

So, in both cases, the OR is the same.

Hence, perhaps this is the maximum possible.

So, perhaps assigning 1 to n-1 numbers, and k - (n-1) to the last number, is a good strategy.

But, is there a better strategy?

Wait, maybe I can assign numbers that cover more bits, to maximize the number of 1s in the OR.

For example, assign numbers that have multiple bits set, but ensure that different bits are set across the numbers.

But, in practice, it seems that assigning 1 to n-1 numbers and k - (n-1) to the last number achieves the same OR as more carefully chosen numbers.

Hence, perhaps this is a valid approach.

Now, let's look at the provided program and see if it implements this logic correctly.

Looking at the code:

t = int(input())

results = []

for _ in range(t):

(n, k) = map(int, input().split())

result = func_1(n, k)

results.append(' '.join(map(str, result)))

print('\n'.join(results))

def func_1(n, k):

nums = [0] * n

for i in range(n):

nums[i] = (1 << k.bit_length() - 1) - 1

k -= nums[i]

nums[-1] += k

return nums

So, the function func_1 takes n and k, initializes a list of n zeros, then in a loop, assigns to each nums[i] the value (1 << k.bit_length() - 1) - 1, which is essentially the largest number with k.bit_length() - 1 bits set to 1.

Wait, (1 << b) - 1 is a number with b bits set to 1.

Wait, (1 << b) is 1 shifted left by b positions, which is 100...0 in binary, with b zeros.

Subtracting 1 gives 011...1, with b ones.

But in the code, it's (1 << k.bit_length() - 1) - 1.

Wait, k.bit_length() gives the number of bits in k.

So, k.bit_length() - 1 would be one less than that.

So, (1 << (k.bit_length() - 1)) - 1 is a number with (k.bit_length() - 1) bits set to 1.

So, for each of the n numbers, it assigns this value to nums[i], subtracts it from k, and finally adds the remainder to the last number.

Wait, but I'm not sure if this achieves the goal of maximizing the number of 1s in the OR.

Let's test this with the second test case: n=2, k=3.

k.bit_length() is 2, since 3 is 11 in binary, which has 2 bits.

So, (1 << (2 - 1)) - 1 = (1 << 1) - 1 = 2 - 1 = 1.

So, nums[0] = 1, k -=1 → k=2

nums[1] =1, k -=1 → k=1

Then, nums[-1] +=k → nums[1] +=1 → nums[1]=2

So, the sequence is [1,2], which matches the example.

Their OR is 3, which has two 1s, which is correct.

Another test case: n=1, k=5

nums[0] = (1 << 3 -1) -1 = (1 << 2) -1 =4 -1=3

k -=3 → k=2

nums[-1] +=2 → nums[0]=5

So, sequence is [5], which matches the first test case.

Another test case: n=2, k=5

nums[0]= (1 << 3 -1) -1=3

k -=3 → k=2

nums[1]=3

k -=3 → k=-1

nums[-1] +=-1 → nums[1]=3 + (-1)=2

So, sequence is [3,2], which is different from the example [5,0], but their OR is 3|2=3, which is same as [5,0]'s OR.

So, same number of 1s in the OR.

Hence, it seems correct.

Fourth test case: n=6, k=51

k.bit_length() is 6, since 51 is 110011, which has 6 bits.

So, (1 << 5) -1=31

So, assign 31 to each of the 6 numbers, subtracting from k each time.

k=51 -31=20

Then nums[1]=31, k=20-31=-11

nums[2]=31 + (-11)=20

So, sequence is [31,31,20,0,0,0], but sum is 31+31+20=82, which is more than 51.

Wait, that can't be right.

Wait, the loop is:

for i in range(n):

nums[i] = (1 << k.bit_length() -1) -1

k -= nums[i]

nums[-1] +=k

So, in this case:

n=6, k=51

(1 << 5) -1=31

Assign nums[0]=31, k=51-31=20

nums[1]=31, k=20-31=-11

nums[2]=31, k=-11-31=-42

nums[3]=31, k=-42-31=-73

nums[4]=31, k=-73-31=-104

nums[5]=31, k=-104-31=-135

Then, nums[-1] +=k → nums[5] += -135 → nums[5]=31-135=-104

So, the sequence is [31,31,31,31,31,-104]

But, this doesn't make sense, since numbers are supposed to be non-negative.

Moreover, the sum is 31+31+31+31+31-104=155-104=51, which is correct.

But, the OR is 31 |31|31|31|31|-104

Wait, but negative numbers have all bits set, so the OR would be all bits set, which is -1 in two's complement.

But, the problem specifies non-negative integers.

So, this approach seems flawed because it can produce negative numbers.

Hence, the program is incorrect because it can generate negative numbers when k is not large enough.

Wait, in this case, k=51, and n=6.

The approach assigns 31 to each of the 6 numbers, and then adjusts the last number to compensate for the over-subtraction.

But, 31*6=186, which is way more than 51.

Hence, k becomes negative, and the last number becomes negative.

But, the problem requires non-negative integers.

Hence, this approach is flawed.

So, the program is incorrect.

Alternative approach:

To maximize the number of 1s in the OR, we need to set as many bits as possible in at least one of the numbers.

Hence, we can assign numbers that have unique bits set, up to the number of bits in k.

But, if n is larger than the number of bits set in k, we can still assign numbers with those bits.

Wait, but in the fourth test case, they set more bits in the OR than are set in k.

Wait, but in that case, the OR had five 1s, while k=51 has four 1s.

Hence, it's possible to set more bits in the OR than are set in k.

Wait, but how?

Because when you OR numbers, a bit in the OR is set if at least one number has that bit set.

Hence, even if a bit isn't set in k, if one of the numbers has that bit set, it's set in the OR.

But, the sum of the numbers is still k.

Hence, perhaps I can assign numbers with some bits set, as long as their sum is k.

Hence, perhaps I can assign numbers with single bits set, up to the sum allowed.

Wait, but in practice, it's tricky.

An alternative approach is to assign the smallest possible numbers to n-1 numbers, and assign the remainder to the last number.

For example, assign 1 to n-1 numbers, and k - (n-1) to the last number.

In this way, the OR is 1 |1 |...|1 | (k - n +1)

Which is k - n +1, if k - n +1 is larger than 1.

But, in the fourth test case, assigning [1,1,1,1,1,46], OR is 47, which has five 1s.

Same as the example.

Hence, perhaps this is a valid approach.

Hence, perhaps the correct approach is to assign 1 to n-1 numbers, and k - (n-1) to the last number.

Hence, the sequence would be [1,1,1,...,1, k - n +1]

In this way, the OR is 1 |1 |...|1 | (k - n +1)

Which is effectively k - n +1, if k - n +1 is larger than 1.

But, in the first test case, n=1, k=5, so [5], OR is 5, which has two 1s.

In the second test case, n=2, k=3, [1,2], OR is 3, which has two 1s.

In the third test case, n=2, k=5, [1,4], OR is 5, which has two 1s.

In the fourth test case, n=6, k=51, [1,1,1,1,1,46], OR is 47, which has five 1s.

Hence, this seems to be a correct approach.

Hence, the program should implement this logic: assign 1 to n-1 numbers, and k - n +1 to the last number.

Hence, the function should be:

def func_1(n, k):

return [1] * (n-1) + [k - n +1]

Hence, the given program is incorrect, because it can produce negative numbers, which are invalid.

Hence, the correctness is False.

**Final Solution**

To solve this problem, we need to construct a sequence of `n` non-negative integers that sum up to `k`, and maximize the number of 1s in the binary representation of their bitwise OR.

## Approach

1. **Understanding the Problem**:
   - We need to maximize the number of set bits (1s) in the bitwise OR of `n` numbers that sum to `k`.
   - The bitwise OR of a set of numbers has a bit set if at least one number in the set has that bit set.

2. **Strategy**:
   - To maximize the number of set bits in the OR, we should aim to have as many different bits set across the numbers as possible.
   - Assigning 1 to `n-1` numbers ensures that the least significant bit (LSB) is set in those numbers.
   - Assign the remainder (`k - (n - 1)`) to the last number to make the sum equal to `k`.

3. **Correctness**:
   - By assigning 1 to `n-1` numbers, we set the LSB in those numbers.
   - The last number gets `k - (n - 1)`, which may set additional higher bits.
   - The OR of these numbers will have the bits set from all the numbers, maximizing the number of 1s.

## Solution Code

```python
t = int(input())
results = []
for _ in range(t):
    (n, k) = map(int, input().split())
    result = func_1(n, k)
    results.append(' '.join(map(str, result)))
print('\n'.join(results))

def func_1(n, k):
    if n == 0:
        if k == 0:
            return []
        else:
            raise ValueError("Cannot create a sequence of 0 numbers that sum to a positive k")
    if n == 1:
        return [k]
    if k < n - 1:
        raise ValueError("Cannot create a sequence of n numbers that sum to k if k < n-1")
    return [1] * (n - 1) + [k - (n - 1)]
```

## Explanation

- **Input Handling**:
  - Read the number of test cases `t`.
  - For each test case, read `n` and `k`.

- **Function `func_1(n, k)`**:
  - If `n` is 0, return an empty list only if `k` is 0; otherwise, raise an error.
  - If `n` is 1, return `[k]`.
  - If `k < n - 1`, it's impossible to assign 1 to each of the first `n-1` numbers, so raise an error.
  - Otherwise, create a list with `n-1` ones and set the last number to `k - (n - 1)`.

- **Output**:
  - Collect results for all test cases and print them, each sequence on a new line.

This approach ensures that we maximize the number of set bits in the bitwise OR while maintaining the sum of the sequence as `k`.