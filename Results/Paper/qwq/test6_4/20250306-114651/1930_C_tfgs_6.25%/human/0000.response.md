Alright, I've got this problem to solve. Let's see what it's asking for. So, Stack has an array `a` of length `n`, and an empty set `S`. He performs an operation `n` times, where in each operation, he selects an index `i` from the current array `a`, inserts `a[i] + i` into the set `S`, and then deletes `a[i]` from `a`. After `n` operations, `a` will be empty, and `S` will contain `n` elements, but since it's a set, there might be duplicates if `a[i] + i` values coincide for different `i`s.

Then, Stack constructs a new array `b` which is `S` sorted in decreasing order. My task is to find the lexicographically largest possible `b` that Stack can make by choosing the indices wisely in each operation.

First, I need to understand what lexicographically largest means. It means that when comparing two arrays, the one that has the largest element at the first position where they differ is considered larger. So, to maximize `b`, I need to make sure that the largest possible values are as front as possible in `b`.

Given that `b` is sorted in decreasing order, the largest values should naturally be at the front. But since `S` is a set and only contains unique elements, I need to maximize the unique values inserted into `S`.

Wait, but the problem says that `S` is a set, so it cannot contain duplicates. That means if `a[i] + i` is already in `S`, inserting it again won't add a new element. So, the challenge is to select indices in such a way that the unique values in `S` are maximized, and among those, the arrangement in `b` is lexicographically largest.

Let me think about how to approach this.

Option 1: Always pick the largest available `a[i] + i`.

But I need to consider that after deleting `a[i]`, the indices of elements to the right of `a[i]` shift left by one. So, the indices change dynamically as elements are deleted.

This seems complicated to handle directly. Maybe there's a smarter way.

Option 2: Consider all possible `a[i] + i` values and their frequencies, then select the unique ones in a way that maximizes `b`.

Wait, but I have to perform exactly `n` operations, and in each operation, I insert `a[i] + i` into `S`, but `S` only keeps unique values. So, if I insert the same value multiple times, it only counts once.

So, the final `S` could have fewer than `n` unique values, depending on the array `a`.

But the problem says that after `n` operations, `a` will be empty, and `S` will contain the unique `a[i] + i` values that were inserted.

Then, `b` is sorted in decreasing order.

I need to maximize `b` lexicographically.

I need to maximize the first element of `b`, then the second, and so on.

Given that `b` is sorted in decreasing order, the first element is the largest unique value in `S`, the second is the second largest, and so on.

So, to maximize `b` lexicographically, I need to maximize the largest unique values in `S`.

Wait, but since `S` is a set and only contains unique values, I need to select indices in such a way that the unique `a[i] + i` values are maximized, and arranged in decreasing order.

But I have to perform exactly `n` operations, inserting `a[i] + i` each time, but only unique values are kept in `S`.

Wait, but I have to perform `n` operations, but `S` might have fewer than `n` unique values.

Wait, but the problem says that `b` is sorted in decreasing order, and it's `S` sorted in decreasing order.

But I need to make `b` as large as possible lexicographically.

I think the key is to maximize the unique values in `S`, and arrange them in decreasing order.

But I need to maximize `b`, which is `S` sorted in decreasing order.

So, to maximize `b`, I need to maximize the largest elements in `S`.

Wait, but `S` contains unique values.

So, I need to select `a[i] + i` values that are as large as possible, and unique.

But I have to perform `n` operations, but `S` might have fewer than `n` unique values.

Wait, but the problem says to output `b`, which is `S` sorted in decreasing order.

But `b` can have fewer than `n` elements if there are duplicates in `S`.

But the problem says to output `b`, which is `S` sorted in decreasing order.

So, I need to maximize `b` lexicographically.

I think the way to do this is to select the operations in such a way that the unique `a[i] + i` values are maximized, and arranged in decreasing order.

Wait, but I have to perform `n` operations, but only unique values are kept in `S`.

So, perhaps I need to select the operations in a way that maximizes the number of unique `a[i] + i` values, and arranges them in decreasing order.

But I'm not sure.

Let me look at the example.

In the first test case:

n = 2

a = [2, 1]

Option 1: Select i=1 first:

Insert a[1] + 1 = 2 + 1 = 3 into S.

Delete a[1], so a becomes [1].

Then, select i=1 again:

Insert a[1] + 1 = 1 + 1 = 2 into S.

S = {3, 2}

b = [3, 2]

Option 2: Select i=2 first:

Insert a[2] + 2 = 1 + 2 = 3 into S.

Delete a[2], so a becomes [2].

Then, select i=1:

Insert a[1] + 1 = 2 + 1 = 3 into S.

But 3 is already in S, so no change.

S = {3}

b = [3]

So, [3,2] is lexicographically larger than [3].

Hence, the better choice is to select i=1 first.

Another test case:

n = 5

a = [1, 100, 1000, 1000000, 1000000000]

If I select the last element each time:

First, select i=5:

Insert a[5] + 5 = 1000000000 + 5 = 1000000005

Delete a[5], a becomes [1,100,1000,1000000]

Second, select i=4:

Insert a[4] + 4 = 1000000 + 4 = 1000004

Delete a[4], a becomes [1,100,1000]

Third, select i=3:

Insert a[3] + 3 = 1000 + 3 = 1003

Delete a[3], a becomes [1,100]

Fourth, select i=2:

Insert a[2] + 2 = 100 + 2 = 102

Delete a[2], a becomes [1]

Fifth, select i=1:

Insert a[1] + 1 = 1 + 1 = 2

Delete a[1], a is empty.

S = {1000000005, 1000004, 1003, 102, 2}

b = [1000000005, 1000004, 1003, 102, 2]

This seems to be the optimal choice.

So, it seems that selecting the operation that gives the largest possible new unique value in each step leads to the lexicographically largest `b`.

So, the strategy is to prioritize inserting the largest possible unique `a[i] + i` values into `S`.

But how to implement this efficiently, especially considering that `n` can be up to 3*10^5, and the sum of `n` over all test cases does not exceed 3*10^5.

I need an efficient algorithm, probably O(n log n) per test case.

Let me think about it.

First, I can compute all possible `a[i] + i` values.

Wait, but in each operation, the array `a` changes as elements are deleted, so the indices change.

But in the example, selecting the last element each time seems to give the largest unique values.

Wait, perhaps if I sort the `a[i] + i` values in decreasing order and select them in that order, as long as they are unique.

But I need to consider that when I select an element, the indices of the elements to the right change.

This seems tricky.

Is there a way to compute the optimal `S` without simulating the operations?

Let me consider that the set `S` will contain unique values of `a[i] + i`, and I need to maximize the lexicographically largest `b`, which is `S` sorted in decreasing order.

I need to maximize the largest element in `S`, then the second largest, and so on.

So, to maximize `b`, I need to maximize the number of large unique values in `S`.

I think that means I should aim to include as many large `a[i] + i` values as possible in `S`, avoiding duplicates.

Wait, but `a[i] + i` can be the same for different `i`s, so I need to handle duplicates.

Also, since deleting an element shifts the indices of the elements to the right, which affects future `a[j] + j` values.

This seems complicated to handle directly.

Is there a better way to model this problem?

Let me consider that each `a[i] + i` is computed based on the current state of the array `a`.

But perhaps I can precompute all possible `a[i] + i` values if I fix the order in which elements are deleted.

Wait, but the order in which elements are deleted affects the `i` values for the remaining elements.

This seems too intertwined.

Maybe I can think of it differently.

Suppose I have the initial array `a` with indices from 1 to n.

When I select an index `i`, I insert `a[i] + i` into `S`, and then delete `a[i]`, shifting the indices of the elements to the right of `i` by one to the left.

I need to perform this `n` times.

I need to choose the sequence of `i`s to maximize the unique values in `S`, sorted in decreasing order.

This seems similar to selecting elements in a specific order to maximize some value.

Perhaps I can model this as selecting elements in decreasing order of `a[i] + i`, but considering the shifting of indices.

But that seems too vague.

Let me consider that the value inserted is `a[i] + i`, and I need unique values in `S`.

So, perhaps I can collect all possible `a[i] + i` values, find the unique ones, and sort them in decreasing order.

But the issue is that selecting one `a[i] + i` might prevent me from selecting another `a[j] + j` if deleting `a[i]` affects the index of `a[j]`.

This seems too convoluted.

Maybe I need to consider that the set `S` will contain unique `a[i] + i` values, and I need to maximize the multiset of these unique values.

Wait, but `S` is a set, so it only contains unique values.

I need to maximize the sorted list of these unique values in decreasing order.

I need to maximize the largest element, then the second largest, and so on.

Perhaps I can think in terms of selecting the largest possible unique `a[i] + i` values first.

But I need to ensure that I can select `n` operations, even if some `a[i] + i` values are the same.

Wait, but since `S` is a set, duplicates are not allowed.

So, if multiple `a[i] + i` are the same, only one will be in `S`.

Hence, the size of `S` can be at most the number of unique `a[i] + i` values.

But I have to perform `n` operations, so I need to insert `n` values into `S`, but only unique ones are kept.

Hence, if there are duplicates, they are only counted once.

But in the end, `b` is `S` sorted in decreasing order.

I need to maximize `b` lexicographically.

I need to maximize the first element, then the second, and so on.

So, to maximize `b`, I need to maximize the largest element in `S`, then the second largest, and so on, up to the size of `S`.

But the size of `S` can be less than `n` if there are duplicates.

But the problem seems to suggest that `b` can have up to `n` elements, but in reality, it can have fewer if there are duplicates.

But in the first example, selecting i=1 first gives S={3,2}, and selecting i=2 first gives S={3}.

Hence, [3,2] is better than [3].

So, in general, I need to select operations in a way that maximizes the number of unique large values in `S`.

This seems tricky.

Let me think about it differently.

Suppose I compute all possible `a[i] + i` values for the initial array.

Then, if I select a particular `a[i] + i`, I need to consider how it affects the remaining elements and their indices.

But this seems too involved.

Is there a smarter way?

Wait, perhaps I can consider that the value `a[i] + i` is fixed for each `i`, and the only thing that changes is the indices of the elements to the right when I delete an element.

But maybe I can precompute all possible `a[i] + i` values and sort them, then select the largest unique ones.

But I need to make sure that selecting one doesn't prevent me from selecting another.

Wait, perhaps I can collect all `a[i] + i` values, sort them in decreasing order, and then select the largest unique ones, filling in the duplicates by decrementing.

Wait, maybe I can look at the frequency of each `a[i] + i` value.

If a value appears multiple times, I can only include it once in `S`.

But I need to maximize `b`, which is `S` sorted in decreasing order.

So, I should include the highest unique `a[i] + i` values first, and for duplicates, I can adjust by decrementing.

Wait, perhaps I need to consider that for each unique `a[i] + i`, I can include it once in `S`, and for duplicates, I can include them by decrementing to make them unique.

But I need to maximize `b`, so I should maximize the higher values.

Let me try to formalize this.

First, compute all `a[i] + i` values.

Then, find the unique values among them.

Sort these unique values in decreasing order.

This would be the base of `b`.

Then, for the duplicates, I can insert values that are one less than the original, ensuring they are unique.

Wait, perhaps I can think in terms of the frequency of each `a[i] + i` value.

If a value appears multiple times, I can include it only once in `S`.

But to maximize `b`, I need to maximize the number of large unique values.

Hence, for each value that appears multiple times, I can include it once, and for the extra occurrences, I can include values that are one less, provided they don't conflict with existing values.

Wait, maybe this is similar to assigning unique ranks to the values, starting from the highest.

But I'm getting a bit confused.

Let me look at the sample input and output again.

Sample Input:

3

2

2 1

5

1 100 1000 1000000 1000000000

3

6 4 8

Sample Output:

3 2

1000000005 1000004 1003 102 2

11 7 6

In the first test case:

n=2, a=[2,1]

Select i=1: insert 2+1=3, delete a[1], a becomes [1]

Select i=1: insert 1+1=2, delete a[1], a is empty

S={3,2}, b=[3,2]

Alternatively, select i=2: insert 1+2=3, delete a[2], a becomes [2]

Select i=1: insert 2+1=3, delete a[1], a is empty

S={3}, b=[3]

Hence, [3,2] is better than [3].

In the second test case:

n=5, a=[1,100,1000,1000000,1000000000]

Select i=5: insert 1000000000+5=1000000005, delete a[5], a=[1,100,1000,1000000]

Select i=4: insert 1000000+4=1000004, delete a[4], a=[1,100,1000]

Select i=3: insert 1000+3=1003, delete a[3], a=[1,100]

Select i=2: insert 100+2=102, delete a[2], a=[1]

Select i=1: insert 1+1=2, delete a[1], a is empty

S={1000000005,1000004,1003,102,2}, b=[1000000005,1000004,1003,102,2]

In the third test case:

n=3, a=[6,4,8]

Select i=1: insert 6+1=7, delete a[1], a=[4,8]

Select i=2: insert 8+1=9, delete a[2], a=[4]

Select i=1: insert 4+1=5, delete a[1], a is empty

S={7,9,5}, b=[9,7,5]

But the sample output is 11,7,6.

Wait, perhaps I made a mistake in selecting the operations.

Wait, if I select i=3 first:

Insert 8+3=11, delete a[3], a=[6,4]

Select i=2: insert 4+2=6, delete a[2], a=[6]

Select i=1: insert 6+1=7, delete a[1], a is empty

S={11,6,7}, b=[11,7,6]

Which matches the sample output.

So, selecting i=3 first gives a better `b`.

Hence, selecting the operation that gives the highest `a[i] + i` first seems beneficial.

So, perhaps sorting the `a[i] + i` values in decreasing order and selecting them in that order, while ensuring uniqueness, is the way to go.

But I need to handle duplicates properly.

Let me think about how to implement this.

First, compute all `a[i] + i` values.

Then, sort them in decreasing order.

Then, iterate through this sorted list, keeping track of the unique values and their frequencies.

Then, construct `b` by taking the unique values in order, and for duplicates, adjust by decrementing to make them unique.

Wait, but I need to make sure that the adjusted values don't conflict with existing values in `S`.

This seems a bit tricky.

Let me consider using a frequency counter for the `a[i] + i` values.

Then, sort the unique values in decreasing order.

Then, for each unique value, I can include it in `b` up to its frequency, but since `S` is a set, I can only include it once.

But I need to maximize `b`, so I need to maximize the number of large unique values.

Hence, for each unique value, I can include it once in `b`, and for extra frequencies, I can include values that are one less than the original, ensuring they don't already exist in `S`.

Wait, but this might lead to overlaps.

Perhaps a better way is to use a priority queue or sort the `a[i] + i` values in decreasing order, and then assign each unique value to `b`, and for duplicates, assign the next available unique value that is less than the original.

But this sounds complicated.

Let me consider the following approach:

1. Compute all `a[i] + i` values.

2. Sort these values in decreasing order.

3. Initialize an empty set `S`.

4. Iterate through the sorted list, and for each value, if it's not already in `S`, add it to `S`; if it is, decrement it by 1 and check again until it's unique.

5. Continue this process until `S` has `n` unique values.

6. Sort `S` in decreasing order to get `b`.

But this might not be efficient enough, especially since `n` can be up to 3*10^5, and I have to handle up to 10^4 test cases.

I need a more efficient way.

Let me think about the frequency of each `a[i] + i` value.

Suppose I have a frequency counter for all `a[i] + i` values.

Then, I can sort the unique values in decreasing order.

Then, for each unique value, I can include it in `S`, and for extra frequencies, I can include value-1, value-2, etc., as long as they are unique.

But I need to make sure that these adjusted values don't overlap with existing values in `S`.

This seems similar to assigning ranks with ties broken by unique adjustments.

Wait, perhaps I can use the concept of "next available unique value".

So, for each unique `a[i] + i`, I can include it in `S`, and for duplicates, I can include the next lower unique value that isn't already in `S`.

But this still seems inefficient for large `n`.

I need a better way.

Let me consider that the optimal `b` is the sequence of unique `a[i] + i` values sorted in decreasing order, with duplicates handled by decrementing to the next available unique value.

So, perhaps I can sort the `a[i] + i` values in decreasing order, and then for each group of duplicates, assign the top value to the first occurrence, and for subsequent occurrences, assign the next lower unique value that isn't already assigned.

But this still seems too slow for large `n`.

I need a more efficient algorithm.

Let me consider using a frequency counter and sorting the unique values in decreasing order.

Then, for each unique value, I can include it in `b`, and for extra frequencies, I can include value-1, value-2, etc., ensuring they are unique.

But I need to make sure that I don't create overlaps with existing values.

Wait, perhaps I can iterate through the sorted unique values and assign as many as their frequency allows, decrementing for duplicates.

Let me try to formalize this.

Step 1: Compute all `a[i] + i` values.

Step 2: Create a frequency counter for these values.

Step 3: Sort the unique values in decreasing order.

Step 4: Initialize an empty list `b`.

Step 5: For each unique value in decreasing order:

a. If the frequency is 1, add the value to `b`.

b. If the frequency is greater than 1, add the value to `b`, and for the extra frequencies, add value-1, value-2, etc., ensuring they are unique and not already in `b`.

Step 6: Sort `b` in decreasing order.

But I need to make sure that the decremented values don't overlap with existing values in `b`.

This seems tricky to implement efficiently.

Let me think about it differently.

Suppose I sort the unique `a[i] + i` values in decreasing order.

Then, for each unique value, I can include it in `b`, and for duplicates, I can include the next lower unique value that isn't already in `b`.

But again, this seems inefficient.

Wait, perhaps I can use a priority queue to keep track of the next available unique value for each group of duplicates.

But I'm not sure.

Let me consider that the optimal `b` is composed of the largest unique `a[i] + i` values, with duplicates handled by decrementing to the next available unique value.

Hence, I can sort the unique `a[i] + i` values in decreasing order, and for each group of duplicates, assign the highest available unique value, then the next highest, and so on.

But I need to handle the frequencies properly.

Wait, maybe I can use the concept of "ranks" where for each group of duplicates, I assign the highest available unique value down to the lowest available unique value.

But this still seems too slow for large `n`.

I need a better approach.

Let me look at the constraints again.

Constraints:

- t (number of test cases): 1 <= t <= 10^4

- n (length of array a per test case): 1 <= n <= 3*10^5

- Sum of n over all test cases <= 3*10^5

Given that the sum of n over all test cases is <= 3*10^5, I need an algorithm that is O(n log n) per test case.

So, sorting is acceptable.

Let me consider the following approach:

1. For each test case, compute all `a[i] + i` values.

2. Create a frequency counter for these values.

3. Sort the unique values in decreasing order.

4. Initialize an empty list `b`.

5. Iterate through the sorted unique values, and for each value, add it to `b` as many times as its frequency allows, decrementing by 1 for each duplicate, ensuring uniqueness.

But ensuring uniqueness while decrementing seems tricky.

Wait, perhaps I can use a set to keep track of the values already in `b`.

Then, for each group of duplicates, start from the original value and decrement until I find a value not already in `b`.

But again, this seems inefficient for large `n`.

I need a smarter way.

Let me consider that for each group of duplicates, I can assign the top value to the first occurrence, the top value -1 to the second occurrence, top value -2 to the third, and so on, ensuring that these assigned values are unique.

But I need to make sure that these assigned values don't overlap with other assigned values.

This seems similar to assigning ranks with ties adjusted by unique decrements.

Wait, perhaps I can use the concept of "adjusted ranks" where for each group of duplicates, I assign the top value, top value -1, and so on, but adjusted to avoid overlaps with other groups.

But this still seems too involved.

Let me consider that the optimal `b` is the sorted unique `a[i] + i` values in decreasing order, with duplicates handled by decrementing to the next available unique value.

Hence, perhaps I can sort the unique `a[i] + i` values in decreasing order, and for each frequency, assign the value minus its position in the frequency group.

Wait, perhaps I can do something like this:

- Sort the unique `a[i] + i` values in decreasing order.

- For each unique value, sort the frequencies in decreasing order.

- Assign the value to the first occurrence, value -1 to the second, value -2 to the third, and so on, ensuring that these assigned values are unique.

But I need to make sure that the assigned values don't overlap with other assigned values.

This seems too error-prone.

Is there a better way?

Let me consider that in the end, `S` contains unique values, and I need to maximize `b`, which is `S` sorted in decreasing order.

Hence, I should aim to have as many large unique values in `S` as possible.

So, perhaps I can select the operations in a way that maximizes the number of large unique `a[i] + i` values in `S`.

Wait, but I have to perform exactly `n` operations, and `S` can have fewer than `n` unique values.

Hence, perhaps I need to maximize the number of unique `a[i] + i` values in `S`, and arrange them in decreasing order.

But I need to maximize `b` lexicographically.

Wait, maybe I need to maximize the sum of the values in `b`, but arranged in decreasing order.

But no, it's specifically lexicographical order.

So, I need to maximize the first element, then the second, and so on.

Given that, perhaps I can prioritize having as many large unique values as possible.

Wait, perhaps I can sort all possible `a[i] + i` values in decreasing order, and then select the top `n` unique values, filling in duplicates by decrementing.

But I need to make sure that the decremented values don't overlap with existing values.

This still seems tricky.

Let me consider that the optimal `b` is composed of the top unique `a[i] + i` values, with duplicates handled by decrementing to the next available unique value.

Hence, perhaps I can sort the unique `a[i] + i` values in decreasing order, and then for each frequency, assign the value minus its position in the frequency group.

Wait, perhaps I can do something like this:

- Compute all `a[i] + i` values.

- Create a frequency counter for these values.

- Sort the unique values in decreasing order.

- Initialize an empty list `b`.

- For each unique value in sorted order:

- For the first occurrence, add the value to `b`.

- For the second occurrence, add value -1 to `b`, ensuring it's unique.

- For the third occurrence, add value -2, ensuring it's unique.

- And so on.

- Finally, sort `b` in decreasing order.

But I need to ensure that the decremented values don't overlap with existing values in `b`.

This seems error-prone, but perhaps it's manageable.

Let me try to implement this logic step by step.

First, compute all `a[i] + i` values.

Then, create a frequency counter.

Then, sort the unique values in decreasing order.

Then, for each unique value, assign the value to the first occurrence, value -1 to the second, value -2 to the third, and so on, ensuring that these assigned values are unique.

Finally, sort the assigned values in decreasing order to get `b`.

This seems plausible, but I need to make sure that the decremented values don't overlap with other assigned values.

Wait, perhaps I can keep track of a set of assigned values and decrement until I find a value not in the assigned set.

But this might be too slow for large `n`.

I need a more efficient way.

Let me consider that for each group of duplicates, I can assign the top value, then top value -1, top value -2, etc., and keep track of the assigned values to ensure uniqueness.

But again, this seems inefficient.

Is there a better approach?

Let me consider that the optimal `b` is the sorted unique `a[i] + i` values in decreasing order, with duplicates handled by decrementing to the next available unique value.

Hence, perhaps I can sort the unique `a[i] + i` values in decreasing order, and for each frequency, assign the value minus its position in the frequency group, ensuring uniqueness.

But I need to make sure that the assigned values are unique across all groups.

This still seems too involved.

Let me consider that in the end, `S` contains unique values, and I need to maximize `b`, which is `S` sorted in decreasing order.

Hence, perhaps I can sort all possible `a[i] + i` values in decreasing order, and then assign the top `n` unique values to `b`, filling in duplicates by decrementing to the next available unique value.

But I need to handle the decrements carefully to avoid overlaps.

This seems too error-prone.

Is there a better way to model this problem?

Wait, perhaps I can think of it as assigning ranks to the `a[i] + i` values, where higher values get higher ranks, and duplicates get slightly lower ranks.

But I need to maximize `b`, which is sorted in decreasing order.

Wait, perhaps I can use a priority queue to select the largest available unique value at each step.

But I need to perform `n` operations, and in each operation, I need to select the largest available unique `a[i] + i` value that hasn't been used yet, or its decremented value if it has been used.

But this still seems too slow.

I need a smarter way.

Let me consider that the optimal `b` is composed of the largest unique `a[i] + i` values, with duplicates handled by decrementing to the next available unique value.

Hence, perhaps I can sort the unique `a[i] + i` values in decreasing order, and for each frequency, assign the value minus its position in the frequency group.

Then, sort these assigned values in decreasing order to get `b`.

But I need to ensure that the assigned values are unique.

Wait, perhaps I can do something like this:

- Compute all `a[i] + i` values.

- Create a frequency counter.

- Sort the unique values in decreasing order.

- Initialize an empty list `b`.

- For each unique value in sorted order:

- For the first occurrence, add the value to `b`.

- For the second occurrence, add value -1 to `b`, but ensure it's unique.

- For the third occurrence, add value -2, ensuring uniqueness.

- And so on.

- Finally, sort `b` in decreasing order.

But I need to handle the uniqueness carefully.

Let me try to implement this logic.

First, compute all `a[i] + i` values.

Then, create a frequency counter.

Then, sort the unique values in decreasing order.

Then, iterate through the sorted unique values, and for each frequency, assign the value minus its position in the frequency group.

But I need to make sure that the assigned values are unique.

To do this efficiently, perhaps I can keep track of the assigned values in a set.

So, here's a step-by-step plan:

1. For each test case:

a. Read `n` and the array `a`.

b. Compute `a[i] + i` for each `i` from 1 to n.

c. Create a frequency counter for these values.

d. Sort the unique values in decreasing order.

e. Initialize an empty set `assigned` to keep track of assigned values.

f. Initialize an empty list `b`.

g. For each unique value in sorted order:

i. For the first occurrence, add the value to `b` and `assigned`.

ii. For the second occurrence, find the