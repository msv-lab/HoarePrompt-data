{
    "total_valid_rows": {
        "value": 97,
        "description": "Total number of valid rows after preprocessing and removing invalid entries."
    },
    "summary fsl": {
        "agreement_count": "50",
        "agreement_percentage": 51.546391752577314,
        "mcc": 0,
        "accuracy": 0.5154639175257731,
        "precision": 0,
        "recall": 0.0,
        "f1_score": 0,
        "balanced_accuracy": 0.5,
        "description": "Agreement analysis between 'summary fsl' and 'original correctness'."
    },
    "naive correctness fsl": {
        "agreement_count": "55",
        "agreement_percentage": 56.70103092783505,
        "mcc": 0.20792959866424848,
        "accuracy": 0.5670103092783505,
        "precision": 0.8571428571428571,
        "recall": 0.1276595744680851,
        "f1_score": 0.2222222222222222,
        "balanced_accuracy": 0.5538297872340425,
        "description": "Agreement analysis between 'naive correctness fsl' and 'original correctness'."
    },
    "simple tree": {
        "agreement_count": "64",
        "agreement_percentage": 65.97938144329896,
        "mcc": 0.31846666811083146,
        "accuracy": 0.6597938144329897,
        "precision": 0.6666666666666666,
        "recall": 0.5957446808510638,
        "f1_score": 0.6292134831460674,
        "balanced_accuracy": 0.6578723404255319,
        "description": "Agreement analysis between 'simple tree' and 'original correctness'."
    },
    "complex tree": {
        "agreement_count": "63",
        "agreement_percentage": 64.94845360824742,
        "mcc": 0.29829787234042554,
        "accuracy": 0.6494845360824743,
        "precision": 0.6382978723404256,
        "recall": 0.6382978723404256,
        "f1_score": 0.6382978723404256,
        "balanced_accuracy": 0.6491489361702127,
        "description": "Agreement analysis between 'complex tree' and 'original correctness'."
    },
    "vanilla": {
        "agreement_count": "55",
        "agreement_percentage": 56.70103092783505,
        "mcc": 0.1394901174339895,
        "accuracy": 0.5670103092783505,
        "precision": 0.5454545454545454,
        "recall": 0.6382978723404256,
        "f1_score": 0.5882352941176471,
        "balanced_accuracy": 0.5691489361702128,
        "description": "Agreement analysis between 'vanilla' and 'original correctness'."
    },
    "vanilla_no_cot": {
        "agreement_count": "62",
        "agreement_percentage": 63.91752577319587,
        "mcc": 0.30457025929110626,
        "accuracy": 0.6391752577319587,
        "precision": 0.75,
        "recall": 0.3829787234042553,
        "f1_score": 0.5070422535211269,
        "balanced_accuracy": 0.6314893617021277,
        "description": "Agreement analysis between 'vanilla_no_cot' and 'original correctness'."
    },
    "summary": {
        "agreement_count": "57",
        "agreement_percentage": 58.76288659793815,
        "mcc": 0.17356116211819228,
        "accuracy": 0.5876288659793815,
        "precision": 0.5777777777777777,
        "recall": 0.5531914893617021,
        "f1_score": 0.5652173913043478,
        "balanced_accuracy": 0.5865957446808511,
        "description": "Agreement analysis between 'summary' and 'original correctness'."
    },
    "simple verify": {
        "agreement_count": "56",
        "agreement_percentage": 57.73195876288659,
        "mcc": 0.1544023959326251,
        "accuracy": 0.5773195876288659,
        "precision": 0.5625,
        "recall": 0.574468085106383,
        "f1_score": 0.5684210526315789,
        "balanced_accuracy": 0.5772340425531914,
        "description": "Agreement analysis between 'simple verify' and 'original correctness'."
    },
    "complex verify": {
        "agreement_count": "56",
        "agreement_percentage": 57.73195876288659,
        "mcc": 0.1544023959326251,
        "accuracy": 0.5773195876288659,
        "precision": 0.5625,
        "recall": 0.574468085106383,
        "f1_score": 0.5684210526315789,
        "balanced_accuracy": 0.5772340425531914,
        "description": "Agreement analysis between 'complex verify' and 'original correctness'."
    },
    "summary verify": {
        "agreement_count": "51",
        "agreement_percentage": 52.57731958762887,
        "mcc": 0.04837415642700526,
        "accuracy": 0.5257731958762887,
        "precision": 0.5116279069767442,
        "recall": 0.46808510638297873,
        "f1_score": 0.4888888888888889,
        "balanced_accuracy": 0.5240425531914894,
        "description": "Agreement analysis between 'summary verify' and 'original correctness'."
    },
    "simple verify fsl": {
        "agreement_count": "50",
        "agreement_percentage": 51.546391752577314,
        "mcc": 0.013752283283450578,
        "accuracy": 0.5154639175257731,
        "precision": 0.5,
        "recall": 0.1702127659574468,
        "f1_score": 0.25396825396825395,
        "balanced_accuracy": 0.5051063829787233,
        "description": "Agreement analysis between 'simple verify fsl' and 'original correctness'."
    },
    "complex verify fsl": {
        "agreement_count": "57",
        "agreement_percentage": 58.76288659793815,
        "mcc": 0.19156825134525532,
        "accuracy": 0.5876288659793815,
        "precision": 0.6666666666666666,
        "recall": 0.2978723404255319,
        "f1_score": 0.4117647058823529,
        "balanced_accuracy": 0.578936170212766,
        "description": "Agreement analysis between 'complex verify fsl' and 'original correctness'."
    },
    "summary verify fsl": {
        "agreement_count": "60",
        "agreement_percentage": 61.855670103092784,
        "mcc": 0.25676554221790676,
        "accuracy": 0.6185567010309279,
        "precision": 0.7083333333333334,
        "recall": 0.3617021276595745,
        "f1_score": 0.4788732394366197,
        "balanced_accuracy": 0.6108510638297873,
        "description": "Agreement analysis between 'summary verify fsl' and 'original correctness'."
    }
}