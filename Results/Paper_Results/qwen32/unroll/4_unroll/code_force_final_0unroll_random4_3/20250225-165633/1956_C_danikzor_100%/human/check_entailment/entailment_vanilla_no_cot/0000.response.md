Correctness: **False**

The given program is incorrect because it attempts to perform operations using the permutation `n, n-1, ..., 1` for both row and column operations. This strategy does not necessarily maximize the sum of the matrix.

To maximize the sum, we need to ensure that each number from 1 to n appears in each row and each column exactly once. One way to achieve this is by filling the matrix with a Latin square, where each row and column contains a permutation of the numbers from 1 to n.

Here's a corrected version of the program that fills the matrix with a simple Latin square pattern:

```python
t = int(input())
for _ in range(t):
    n = int(input())
    max_sum = n * (n * n + 1) // 2
    print(max_sum, 2 * n)
    for i in range(1, n + 1):
        # Fill the i-th row with the permutation (i, i+1, ..., n, 1, 2, ..., i-1)
        row_permutation = list(range(i, n + 1)) + list(range(1, i))
        print(1, i, *row_permutation)
        # Fill the i-th column with the permutation (i, i+1, ..., n, 1, 2, ..., i-1)
        col_permutation = list(range(i, n + 1)) + list(range(1, i))
        print(2, i, *col_permutation)
```

This program ensures that each number from 1 to n appears exactly once in each row and each column, thus achieving the maximum possible sum of \( n \times \frac{n(n+1)}{2} \). The number of operations is exactly \( 2n \), which meets the problem's constraints.