{
    "total_valid_rows": {
        "value": 281,
        "description": "Total number of valid rows after preprocessing and removing invalid entries."
    },
    "summary fsl": {
        "agreement_count": "173",
        "agreement_percentage": 61.56583629893239,
        "mcc": 0.2264881614669758,
        "accuracy": 0.6156583629893239,
        "precision": 0.6428571428571429,
        "recall": 0.4090909090909091,
        "f1_score": 0.5000000000000001,
        "balanced_accuracy": 0.6038743136058572,
        "description": "Agreement analysis between 'summary fsl' and 'original correctness'."
    },
    "naive correctness fsl": {
        "agreement_count": "157",
        "agreement_percentage": 55.871886120996436,
        "mcc": 0.1045998838559763,
        "accuracy": 0.5587188612099644,
        "precision": 0.5909090909090909,
        "recall": 0.19696969696969696,
        "f1_score": 0.29545454545454547,
        "balanced_accuracy": 0.5380821639210901,
        "description": "Agreement analysis between 'naive correctness fsl' and 'original correctness'."
    },
    "simple tree": {
        "agreement_count": "167",
        "agreement_percentage": 59.430604982206404,
        "mcc": 0.18711298930356796,
        "accuracy": 0.594306049822064,
        "precision": 0.5661764705882353,
        "recall": 0.5833333333333334,
        "f1_score": 0.5746268656716418,
        "balanced_accuracy": 0.5936800894854586,
        "description": "Agreement analysis between 'simple tree' and 'original correctness'."
    },
    "complex tree": {
        "agreement_count": "161",
        "agreement_percentage": 57.29537366548043,
        "mcc": 0.1413434399284576,
        "accuracy": 0.5729537366548043,
        "precision": 0.546875,
        "recall": 0.5303030303030303,
        "f1_score": 0.5384615384615384,
        "balanced_accuracy": 0.5705206426682936,
        "description": "Agreement analysis between 'complex tree' and 'original correctness'."
    },
    "vanilla": {
        "agreement_count": "167",
        "agreement_percentage": 59.430604982206404,
        "mcc": 0.18009571746546207,
        "accuracy": 0.594306049822064,
        "precision": 0.5818181818181818,
        "recall": 0.48484848484848486,
        "f1_score": 0.5289256198347108,
        "balanced_accuracy": 0.5880618263168599,
        "description": "Agreement analysis between 'vanilla' and 'original correctness'."
    },
    "vanilla_no_cot": {
        "agreement_count": "171",
        "agreement_percentage": 60.854092526690394,
        "mcc": 0.22139640293592205,
        "accuracy": 0.608540925266904,
        "precision": 0.6774193548387096,
        "recall": 0.3181818181818182,
        "f1_score": 0.43298969072164945,
        "balanced_accuracy": 0.5919768151311775,
        "description": "Agreement analysis between 'vanilla_no_cot' and 'original correctness'."
    },
    "summary": {
        "agreement_count": "170",
        "agreement_percentage": 60.4982206405694,
        "mcc": 0.2060802893513866,
        "accuracy": 0.604982206405694,
        "precision": 0.5813953488372093,
        "recall": 0.5681818181818182,
        "f1_score": 0.5747126436781609,
        "balanced_accuracy": 0.6028828553996339,
        "description": "Agreement analysis between 'summary' and 'original correctness'."
    },
    "simple verify": {
        "agreement_count": "173",
        "agreement_percentage": 61.56583629893239,
        "mcc": 0.22370765018904276,
        "accuracy": 0.6156583629893239,
        "precision": 0.6132075471698113,
        "recall": 0.49242424242424243,
        "f1_score": 0.5462184873949579,
        "balanced_accuracy": 0.6086282285946716,
        "description": "Agreement analysis between 'simple verify' and 'original correctness'."
    },
    "complex verify": {
        "agreement_count": "168",
        "agreement_percentage": 59.7864768683274,
        "mcc": 0.18726736981394557,
        "accuracy": 0.597864768683274,
        "precision": 0.5871559633027523,
        "recall": 0.48484848484848486,
        "f1_score": 0.5311203319502075,
        "balanced_accuracy": 0.5914175310148464,
        "description": "Agreement analysis between 'complex verify' and 'original correctness'."
    },
    "summary verify": {
        "agreement_count": "170",
        "agreement_percentage": 60.4982206405694,
        "mcc": 0.20148470208745906,
        "accuracy": 0.604982206405694,
        "precision": 0.6082474226804123,
        "recall": 0.44696969696969696,
        "f1_score": 0.5152838427947598,
        "balanced_accuracy": 0.5959680699613585,
        "description": "Agreement analysis between 'summary verify' and 'original correctness'."
    },
    "simple verify fsl": {
        "agreement_count": "161",
        "agreement_percentage": 57.29537366548043,
        "mcc": 0.1338707587334264,
        "accuracy": 0.5729537366548043,
        "precision": 0.5625,
        "recall": 0.4090909090909091,
        "f1_score": 0.47368421052631576,
        "balanced_accuracy": 0.5636058572300183,
        "description": "Agreement analysis between 'simple verify fsl' and 'original correctness'."
    },
    "complex verify fsl": {
        "agreement_count": "158",
        "agreement_percentage": 56.22775800711744,
        "mcc": 0.10976914178948559,
        "accuracy": 0.5622775800711743,
        "precision": 0.5529411764705883,
        "recall": 0.3560606060606061,
        "f1_score": 0.43317972350230416,
        "balanced_accuracy": 0.5505135245068131,
        "description": "Agreement analysis between 'complex verify fsl' and 'original correctness'."
    },
    "summary verify fsl": {
        "agreement_count": "166",
        "agreement_percentage": 59.07473309608541,
        "mcc": 0.17358995157206292,
        "accuracy": 0.5907473309608541,
        "precision": 0.6133333333333333,
        "recall": 0.3484848484848485,
        "f1_score": 0.4444444444444444,
        "balanced_accuracy": 0.5769269880008134,
        "description": "Agreement analysis between 'summary verify fsl' and 'original correctness'."
    }
}