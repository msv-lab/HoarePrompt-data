{
    "total_valid_rows": {
        "value": 22,
        "description": "Total number of valid rows after preprocessing and removing invalid entries."
    },
    "summary fsl": {
        "agreement_count": "5",
        "agreement_percentage": 22.727272727272727,
        "mcc": 0,
        "accuracy": 0.22727272727272727,
        "precision": 0,
        "recall": 0.0,
        "f1_score": 0,
        "balanced_accuracy": 0.5,
        "description": "Agreement analysis between 'summary fsl' and 'original correctness'."
    },
    "naive correctness fsl": {
        "agreement_count": "7",
        "agreement_percentage": 31.818181818181817,
        "mcc": -0.15498260496951669,
        "accuracy": 0.3181818181818182,
        "precision": 0.6666666666666666,
        "recall": 0.23529411764705882,
        "f1_score": 0.3478260869565218,
        "balanced_accuracy": 0.41764705882352937,
        "description": "Agreement analysis between 'naive correctness fsl' and 'original correctness'."
    },
    "simple tree": {
        "agreement_count": "16",
        "agreement_percentage": 72.72727272727273,
        "mcc": 0.2235294117647059,
        "accuracy": 0.7272727272727273,
        "precision": 0.8235294117647058,
        "recall": 0.8235294117647058,
        "f1_score": 0.8235294117647058,
        "balanced_accuracy": 0.611764705882353,
        "description": "Agreement analysis between 'simple tree' and 'original correctness'."
    },
    "complex tree": {
        "agreement_count": "16",
        "agreement_percentage": 72.72727272727273,
        "mcc": 0.1005659937148103,
        "accuracy": 0.7272727272727273,
        "precision": 0.7894736842105263,
        "recall": 0.8823529411764706,
        "f1_score": 0.8333333333333333,
        "balanced_accuracy": 0.5411764705882353,
        "description": "Agreement analysis between 'complex tree' and 'original correctness'."
    },
    "vanilla": {
        "agreement_count": "17",
        "agreement_percentage": 77.27272727272727,
        "mcc": 0.2057983021710106,
        "accuracy": 0.7727272727272727,
        "precision": 0.8,
        "recall": 0.9411764705882353,
        "f1_score": 0.8648648648648648,
        "balanced_accuracy": 0.5705882352941176,
        "description": "Agreement analysis between 'vanilla' and 'original correctness'."
    },
    "vanilla_no_cot": {
        "agreement_count": "14",
        "agreement_percentage": 63.63636363636363,
        "mcc": -0.03529411764705882,
        "accuracy": 0.6363636363636364,
        "precision": 0.7647058823529411,
        "recall": 0.7647058823529411,
        "f1_score": 0.7647058823529412,
        "balanced_accuracy": 0.48235294117647054,
        "description": "Agreement analysis between 'vanilla_no_cot' and 'original correctness'."
    },
    "summary": {
        "agreement_count": "16",
        "agreement_percentage": 72.72727272727273,
        "mcc": -0.11834526708278771,
        "accuracy": 0.7272727272727273,
        "precision": 0.7619047619047619,
        "recall": 0.9411764705882353,
        "f1_score": 0.8421052631578947,
        "balanced_accuracy": 0.47058823529411764,
        "description": "Agreement analysis between 'summary' and 'original correctness'."
    },
    "simple verify": {
        "agreement_count": "15",
        "agreement_percentage": 68.18181818181817,
        "mcc": -0.17149858514250885,
        "accuracy": 0.6818181818181818,
        "precision": 0.75,
        "recall": 0.8823529411764706,
        "f1_score": 0.8108108108108107,
        "balanced_accuracy": 0.4411764705882353,
        "description": "Agreement analysis between 'simple verify' and 'original correctness'."
    },
    "complex verify": {
        "agreement_count": "16",
        "agreement_percentage": 72.72727272727273,
        "mcc": 0.1005659937148103,
        "accuracy": 0.7272727272727273,
        "precision": 0.7894736842105263,
        "recall": 0.8823529411764706,
        "f1_score": 0.8333333333333333,
        "balanced_accuracy": 0.5411764705882353,
        "description": "Agreement analysis between 'complex verify' and 'original correctness'."
    },
    "summary verify": {
        "agreement_count": "16",
        "agreement_percentage": 72.72727272727273,
        "mcc": 0.1005659937148103,
        "accuracy": 0.7272727272727273,
        "precision": 0.7894736842105263,
        "recall": 0.8823529411764706,
        "f1_score": 0.8333333333333333,
        "balanced_accuracy": 0.5411764705882353,
        "description": "Agreement analysis between 'summary verify' and 'original correctness'."
    },
    "simple verify fsl": {
        "agreement_count": "7",
        "agreement_percentage": 31.818181818181817,
        "mcc": -0.37625606633113623,
        "accuracy": 0.3181818181818182,
        "precision": 0.6,
        "recall": 0.35294117647058826,
        "f1_score": 0.4444444444444445,
        "balanced_accuracy": 0.27647058823529413,
        "description": "Agreement analysis between 'simple verify fsl' and 'original correctness'."
    },
    "complex verify fsl": {
        "agreement_count": "8",
        "agreement_percentage": 36.36363636363637,
        "mcc": -0.21057989315866538,
        "accuracy": 0.36363636363636365,
        "precision": 0.6666666666666666,
        "recall": 0.35294117647058826,
        "f1_score": 0.46153846153846156,
        "balanced_accuracy": 0.3764705882352941,
        "description": "Agreement analysis between 'complex verify fsl' and 'original correctness'."
    },
    "summary verify fsl": {
        "agreement_count": "11",
        "agreement_percentage": 50.0,
        "mcc": -0.05940885257860046,
        "accuracy": 0.5,
        "precision": 0.75,
        "recall": 0.5294117647058824,
        "f1_score": 0.6206896551724139,
        "balanced_accuracy": 0.4647058823529412,
        "description": "Agreement analysis between 'summary verify fsl' and 'original correctness'."
    }
}