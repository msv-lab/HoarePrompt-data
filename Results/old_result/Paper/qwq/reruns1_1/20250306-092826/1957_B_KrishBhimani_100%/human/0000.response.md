Alright, I have this problem to solve. Let's see what it's asking for. So, I'm given two integers, n and k. I need to construct a sequence of n non-negative integers that sum up to k. Additionally, I need to maximize the number of 1s in the binary representation of the bitwise OR of all these numbers in the sequence.

First, I need to understand what bitwise OR does. Bitwise OR takes two numbers and performs the OR operation on each pair of corresponding bits. For example, 1 OR 2 is 3 because in binary, 1 is 01 and 2 is 10, and OR'ing them gives 11, which is 3.

So, the goal is to have the bitwise OR of all the numbers in the sequence have as many 1s in its binary representation as possible.

Let's consider the constraints:

- n can be from 1 to 2*10^5, which is 200,000.

- k can be up to 10^9, which is a billion.

- The sum of n over all test cases doesn't exceed 2*10^5, so it's manageable.

Given that t can be up to 10^4, which is 10,000, but since n is limited and the total sum of n is limited, we need an efficient solution.

Let's look at the examples to get a better understanding.

First test case:

n = 1, k = 5

So, only one number, which is 5. The bitwise OR is just 5, which in binary is 101, so it has two 1s.

Second test case:

n = 2, k = 3

Possible sequences:

- [1, 2]: sum is 3, bitwise OR is 1 | 2 = 3 (11 in binary), which has two 1s.

- [0, 3]: sum is 3, bitwise OR is 0 | 3 = 3 (11 in binary), same as above.

- [3, 0]: same as above.

- [1, 2]: same as first option.

So, in this case, both [1,2] and [0,3] or [3,0] give the same result.

But the problem says to maximize the number of 1s in the bitwise OR.

Wait, in this case, all options give the same number of 1s in the OR, which is two.

Looking at the fourth test case:

n = 6, k = 51

Output: 3 1 1 32 2 12

Let's verify:

Sum: 3 + 1 + 1 + 32 + 2 + 12 = 51

Bitwise OR: 3 | 1 | 1 | 32 | 2 | 12

Let's compute that step by step:

3 in binary: 00011

1: 00001

OR with 1: 00011

OR with 1: still 00011

OR with 32 (100000): 100111

OR with 2 (00010): 100111

OR with 12 (01100): 101111

So, the final OR is 101111, which has five 1s.

The problem mentions that this is the maximum possible.

So, the strategy seems to be to set as many bits as possible in the OR, while ensuring the sum is k.

Given that, I need to distribute the value k among n numbers such that their OR has as many 1 bits as possible.

Some observations:

- To maximize the number of 1 bits in the OR, we need as many bits to be set to 1 in the OR result as possible.

- The OR of numbers will have a 1 in a particular bit position if at least one of the numbers has a 1 in that bit position.

- So, to maximize the number of 1 bits in the OR, we need to have at least one number contributing a 1 to each bit position.

- However, we also need to ensure that the sum of all numbers is k.

- Additionally, n is the number of elements in the sequence.

Let's think about how to approach this.

One way is to try to set as many bits as possible in the OR by distributing the bits across the numbers in the sequence.

But I need to maximize the number of bits set in the OR, not necessarily the value of the OR.

Wait, actually, maximizing the number of bits set in the OR is equivalent to maximizing the number of distinct bit positions that are set to 1 in at least one of the numbers.

So, the strategy should be to set different bits in different numbers, but ensuring that the sum is k.

Wait, but bits are overlapping across numbers, so I need to be careful.

Wait, no. Since OR combines the bits, a bit is set in the OR if it's set in any of the numbers.

So, to maximize the number of bits set in the OR, I need to have numbers that together set as many distinct bit positions as possible.

In other words, I need to have numbers that, when combined, cover as many bit positions as possible.

But, since we're dealing with non-negative integers and sum is k, I need to distribute k among n numbers such that their combined bitwise OR sets as many bits as possible.

Let me think differently.

Suppose I have n numbers, and I want their OR to have as many 1 bits as possible.

Each number can contribute to setting certain bits in the OR.

To maximize the number of 1 bits in the OR, I should aim to have each number set a unique set of bits, without overlapping, as much as possible.

But, since the numbers sum to k, I need to distribute k among n numbers.

Wait, but the sum is k, which is the sum of the numbers, not their OR.

So, I need to maximize the number of bits set in the OR, while ensuring that the sum of the numbers is k.

This seems a bit tricky.

Let me consider the extreme cases.

If n = 1, then I just have one number, which is k, and the number of 1 bits in its binary representation is fixed.

If n = k, I can set each number to 1, so their sum is k, and their OR is 1, which has only one 1 bit.

But, in the second test case, n=2, k=3, and the output is [1,2], which has OR 3 (two 1 bits), which is better than [0,3] or [3,0], which also have OR 3.

Wait, actually, [0,3] or [3,0] also have OR 3, same as [1,2].

So, in this case, it's the same.

But in the fourth test case, n=6, k=51, and the output is [3,1,1,32,2,12], which have OR 101111, which is 47 in decimal, with five 1 bits.

Wait, but 51 in binary is 110011, which is also five 1 bits.

So, in this case, even [51,0,0,0,0,0] would have OR 51, which is the same number of 1 bits.

So, in this case, it's the same.

But perhaps in some cases, distributing the sum k across multiple numbers allows for a higher number of 1 bits in the OR.

Wait, but in this case, both [51,0,0,0,0,0] and [3,1,1,32,2,12] have OR with five 1 bits.

So, perhaps in this particular case, it's the maximum possible.

But maybe in other cases, distributing k across n numbers can lead to a higher number of 1 bits in the OR.

Wait, let's think about k=3, n=2.

Possible sequences:

- [0,3]: OR 3 (two 1 bits)

- [1,2]: OR 3 (two 1 bits)

- [3,0]: OR 3 (two 1 bits)

- [1,1]: OR 1 (one 1 bit)

- [2,1]: OR 2 (one 1 bit)

Wait, actually, [1,2] has OR 3, which has two 1 bits, which is more than [1,1] or [2,1], which have OR 1 or 2, which have one 1 bit.

So, in this case, [1,2] is better than [1,1] or [2,1].

But [0,3] and [3,0] are also as good as [1,2].

So, in this case, it's still better to have at least one number with higher bits set.

Wait, but in this case, it's the same.

Let me think about another example.

Suppose n=3, k=3.

Possible sequences:

- [1,1,1]: OR 1 (one 1 bit)

- [0,0,3]: OR 3 (two 1 bits)

- [0,1,2]: OR 3 (two 1 bits)

- [1,2,0]: OR 3 (two 1 bits)

So, in this case, [0,0,3] and [0,1,2] and [1,2,0] all have OR with two 1 bits, which is better than [1,1,1] with OR 1.

So, distributing the sum k across the numbers in a way that sets higher bits in some numbers is better.

So, the general strategy seems to be:

- Assign one number to be k, and the rest to be 0.

- This ensures that the OR is k, which has a certain number of 1 bits.

- Alternatively, distribute k among the numbers in a way that their OR has more 1 bits than just k alone.

Wait, but in the previous example, [1,2,0] has OR 3, which has the same number of 1 bits as [0,0,3].

So, in that case, it's the same.

But perhaps in some cases, distributing k can lead to more 1 bits in the OR.

Let me think of an example where distributing k leads to more 1 bits in the OR.

Consider n=2, k=3.

From earlier, [1,2] has OR 3 (two 1 bits), while [0,3] has OR 3 (two 1 bits).

Same number of 1 bits.

Another example: n=3, k=3.

[0,0,3] has OR 3 (two 1 bits), [0,1,2] has OR 3 (two 1 bits).

Same again.

Another example: n=4, k=3.

Possible sequences:

- [0,0,0,3]: OR 3 (two 1 bits)

- [0,0,1,2]: OR 3 (two 1 bits)

- [0,1,1,1]: OR 1 (one 1 bit)

So, again, the best is two 1 bits.

Is there any case where distributing k across more numbers increases the number of 1 bits in the OR?

Let me think.

Suppose n=2, k=1.

Possible sequences:

- [0,1]: OR 1 (one 1 bit)

- [1,0]: OR 1 (one 1 bit)

- [0,1]: same

No improvement.

Another example: n=3, k=1.

Possible sequences:

- [0,0,1]: OR 1 (one 1 bit)

- [0,1,0]: OR 1 (one 1 bit)

- [1,0,0]: OR 1 (one 1 bit)

Again, same.

Another example: n=2, k=2.

Possible sequences:

- [0,2]: OR 2 (one 1 bit)

- [1,1]: OR 1 (one 1 bit)

- [2,0]: OR 2 (one 1 bit)

- [0,2]: same

So, again, same number of 1 bits.

Wait, is there any case where distributing k across more numbers increases the number of 1 bits in the OR?

Let me think carefully.

Suppose n=2, k=5.

Possible sequences:

- [1,4]: OR 5 (two 1 bits)

- [0,5]: OR 5 (two 1 bits)

- [2,3]: OR 3 (two 1 bits)

- [5,0]: OR 5 (two 1 bits)

So, again, same number of 1 bits.

Hmm.

Another example: n=3, k=5.

Possible sequences:

- [1,4,0]: OR 5 (two 1 bits)

- [0,5,0]: OR 5 (two 1 bits)

- [2,3,0]: OR 3 (two 1 bits)

- [1,2,2]: OR 3 (two 1 bits)

- [1,1,3]: OR 3 (two 1 bits)

Again, same number of 1 bits.

Wait, is there a way to get more than two 1 bits in the OR?

Let's see, 5 in binary is 101, which has two 1 bits.

Is there a way to have more than two 1 bits in the OR when k=5?

Let me see: if I choose numbers whose OR sets more bits to 1.

Wait, but k=5 is 101, which only has two 1 bits.

So, the OR can't have more than two 1 bits because the sum is 5, and higher bits would require larger numbers, but the sum would exceed 5.

Wait, no, the OR is about the bits set in any of the numbers, not the sum.

Wait, but the numbers have to sum to k.

Wait, suppose I have n=3, k=5.

Can I have numbers like [1 (001), 2 (010), 4 (100)]? Their sum is 7, which exceeds k=5.

Oh, no, their sum must be exactly k=5.

Wait, so [1,2,2]: sum is 5, OR is 3 (11 in binary, two 1 bits)

[1,4,0]: sum is 5, OR is 5 (101, two 1 bits)

[3,1,1]: sum is 5, OR is 3 (11, two 1 bits)

So, again, same number of 1 bits.

Seems like for k=5, the maximum number of 1 bits in the OR is two.

Wait, but in the fourth test case, n=6, k=51, and the OR has five 1 bits.

51 in binary is 110011, which has four 1 bits, but the explanation says five.

Wait, in the note, it says the OR is 101111, which is 47, which is less than 51.

Wait, 51 is 110011, which is 32 + 16 + 2 + 1 = 110011

But the output is 3,1,1,32,2,12

Let's compute the OR:

3 is 000011

1 is 000001

1 is 000001

32 is 100000

2 is 000010

12 is 001100

OR'ing them:

- Bit 0: 1

- Bit 1: 1

- Bit 2: 1 (from 32)

- Bit 3: 1 (from 12)

- Bit 4: 1 (from 12)

- Bit 5: 0 (none have bit 5 set)

So, OR is 101111, which is 47, which has five 1 bits.

Whereas, if we take [51,0,0,0,0,0], OR is 110011, which has four 1 bits.

So, in this case, distributing k across multiple numbers allowed for more 1 bits in the OR.

Interesting.

So, the strategy is not just to have one number as k and the rest as zero, but to distribute k across multiple numbers in a way that their OR sets more bits to 1.

But how?

In this example, by splitting 51 into 3,1,1,32,2,12, their OR sets more bits to 1 than just having 51 and zeros.

So, how to achieve this in general?

Let's think about it.

Given n numbers that sum to k, we need their OR to have as many 1 bits as possible.

Each number can set some bits in the OR.

To maximize the number of 1 bits in the OR, we need to set as many distinct bits as possible across the numbers.

In other words, we want to cover as many bit positions with at least one 1 across the numbers.

So, the idea is to set different bits in different numbers, if possible.

But, we also need to ensure that the sum is k.

So, perhaps, start by allocating the smallest possible numbers to as many numbers as possible, while still summing to k.

Wait, but that might not be efficient.

Wait, perhaps it's better to allocate numbers that have only one bit set, as much as possible, without exceeding k.

But, if we have n numbers, and we want to set different bits in each number, we can set up to 20 bits (since k can be up to 1e9), but in practice, we need to see.

Let's formalize this.

Let’s consider the binary representation.

We have 32 bits for integers.

We need to select a subset of these bits to be set in the OR.

Each bit can be set by at least one number in the sequence.

The cost of setting a particular bit is the smallest number that has only that bit set.

For example, to set bit 0, the smallest number is 1 (0001)

To set bit 1, the smallest number is 2 (0010)

To set bit 2, the smallest number is 4 (0100)

And so on.

So, to set m distinct bits, the minimal sum is the sum of the smallest numbers that set each of these m bits.

But, we need the sum to be k.

So, perhaps, we need to select as many bits as possible, and assign the minimal numbers to set those bits, and adjust the remaining sum with other numbers.

Wait, perhaps it's a greedy approach.

Start by assigning the smallest possible numbers to set new bits, and use the remaining sum for other numbers.

Let’s think step by step.

Initialize an array of n numbers, all set to 0.

Initialize a mask of OR as 0.

Iterate through the bits from highest to lowest, or lowest to highest.

Try to set a new bit in one of the numbers, if possible.

Ensure that the sum remains k.

But perhaps there is a better way.

Looking back at the provided code, let's see what it does.

The code is:

def func():

for _ in range(int(input())):

l1 = input().split()

(n, k) = list(map(int, l1))

if n == 1:

print(k)

else:

arr = []

k0 = k

i = 0

ans = []

temp = 1

while True:

if temp * 2 < k:

temp *= 2

i += 1

else:

break

ans.append((1 << i) - 1)

ans.append(k - sum(ans))

ans += [0] * (n - len(ans))

print(*ans)



So, for each test case, it reads n and k.

If n is 1, it simply prints k.

Else, it initializes some variables.

It seems to find the largest power of 2 less than k, and then does something.

Wait, it sets temp =1, then multiplies by 2 while temp*2 <k.

So, it finds the smallest i such that temp = 2^i > k/2.

Wait, temp starts at 1.

While temp*2 <k, temp *=2, i+=1.

So, it finds the largest i where 2^i <k.

Then, ans.append((1<<i)-1), which is 2^(i+1)-1 -1 = 2^(i+1)-2.

Wait, no, (1<<i)-1 is (2^i)-1.

Then, ans.append(k - sum(ans)).

Then, ans += [0]*(n-len(ans)).

Wait, let's see with an example.

Take n=2, k=3.

temp=1, 1*2=2 <3, so temp=2, i=1.

Then, ans.append(2^1 -1 =1), ans=[1]

Then, ans.append(3 -1=2), ans=[1,2]

Then, ans += [0]*(2-2=0), so final ans=[1,2]

Which matches the sample output.

Another example: n=6, k=51

Find temp=1, 1*2=2<51, temp=2,i=1

2*2=4<51, temp=4,i=2

4*2=8<51, temp=8,i=3

8*2=16<51, temp=16,i=4

16*2=32<51, temp=32,i=5

32*2=64>51, stop.

ans.append(2^5 -1=31), ans=[31]

ans.append(51-31=20), ans=[31,20]

Then, ans += [0]*(6-2=4), so ans=[31,20,0,0,0,0]

But in the sample output, it's [3,1,1,32,2,12]

Wait, so this is different.

But in the explanation, they have [3,1,1,32,2,12], which sums to 51 and OR is 47 with five 1 bits.

Whereas, [31,20,0,0,0,0] sums to 51, OR is 31|20|0|0|0|0 = 31|20 = 31 OR 20.

31 is 11111, 20 is 10100, OR is 11111, which is 31, which has five 1 bits.

Same as the sample output's OR of 47 (101111, also five 1 bits).

So, in this case, both have five 1 bits.

So, perhaps multiple solutions exist.

But, in this approach, it seems to aim for a similar number of 1 bits in the OR.

Let me see how this algorithm works in general.

It seems to find the largest i where 2^i <k, then set ans.append(2^i -1), which is a number with i bits set to 1.

Then, append (k - (2^i -1)), which is the remaining sum.

Then, fill the rest with zeros.

So, in terms of maximizing the number of 1 bits in the OR, it sets one number to have i bits set, and another to have the remaining sum.

In the first number, it has i bits set, and the second number has its own bits set.

The OR will have the bits set from either number.

So, if the second number has bits that are not set in the first number, those will also be set in the OR.

But in the previous example, [31,20], 31 is 11111 and 20 is 10100, so OR is 11111, which is 31, which has five 1 bits.

Similarly, in the sample output, [3,1,1,32,2,12], OR is 101111, which is also five 1 bits.

So, both achieve the same number of 1 bits in the OR.

Is this always optimal?

Let me see.

Take n=2, k=6.

According to the algorithm:

temp=1, 1*2=2<6, temp=2,i=1

2*2=4<6, temp=4,i=2

4*2=8>6, stop.

ans.append(2^2 -1=3), ans=[3]

ans.append(6-3=3), ans=[3,3]

Then, ans += [0]*(2-2=0), so ans=[3,3]

OR is 3|3=3, which is 11 in binary, two 1 bits.

But, if I choose [1,5], OR is 1|5=5 (101), which has two 1 bits.

Same as [3,3].

Another option: [2,4], OR is 2|4=6 (110), which has two 1 bits.

Same number of 1 bits.

Seems like it's the maximum possible.

Because 6 is 110, which has two 1 bits.

There's no way to have more than two 1 bits in the OR.

Because k=6, and any number larger than 6 would exceed the sum.

Wait, but if I choose [1,5], OR is 5 (101), two 1 bits.

[2,4]: OR 6 (110), two 1 bits.

[3,3]: OR 3 (011), two 1 bits.

So, all options give two 1 bits.

Seems like it's the maximum.

Another example: n=3, k=6.

Possible sequences:

- [2,2,2]: OR 2 (10), one 1 bit

- [1,2,3]: OR 3 (11), two 1 bits

- [0,0,6]: OR 6 (110), two 1 bits

- [1,1,4]: OR 5 (101), two 1 bits

So, again, the maximum number of 1 bits in the OR is two.

The algorithm would do:

temp=1, 1*2=2<6, temp=2,i=1

2*2=4<6, temp=4,i=2

4*2=8>6, stop.

ans.append(2^2 -1=3), ans=[3]

ans.append(6-3=3), ans=[3,3]

ans += [0]*(3-2=1), so ans=[3,3,0]

OR is 3|3|0=3 (11), two 1 bits.

Which is the same as other options.

Seems optimal.

Another example: n=4, k=7.

Algorithm:

temp=1, 1*2=2<7, temp=2,i=1

2*2=4<7, temp=4,i=2

4*2=8>7, stop.

ans.append(2^2 -1=3), ans=[3]

ans.append(7-3=4), ans=[3,4]

ans += [0]*(4-2=2), so ans=[3,4,0,0]

OR is 3|4|0|0 = 7 (111), three 1 bits.

Is there a better option?

[1,1,1,4]: OR 1|1|1|4=5 (101), two 1 bits.

[1,2,2,2]: OR 1|2|2|2=3 (11), two 1 bits.

[3,4,0,0]: OR 7 (111), three 1 bits.

So, [3,4,0,0] has three 1 bits, which is better than other options.

Seems optimal.

Another example: n=5, k=10.

Algorithm:

temp=1, 1*2=2<10, temp=2,i=1

2*2=4<10, temp=4,i=2

4*2=8<10, temp=8,i=3

8*2=16>10, stop.

ans.append(2^3 -1=7), ans=[7]

ans.append(10-7=3), ans=[7,3]

ans += [0]*(5-2=3), so ans=[7,3,0,0,0]

OR is 7|3|0|0|0=7 (111), three 1 bits.

Is there a better option?

[1,1,1,1,6]: OR 1|1|1|1|6=7 (111), three 1 bits.

[2,2,2,2,2]: OR 2|2|2|2|2=2 (10), one 1 bit.

[1,2,3,4,0]: OR 1|2|3|4|0=7 (111), three 1 bits.

So, again, [7,3,0,0,0] achieves the maximum of three 1 bits.

Seems optimal.

Another example: n=2, k=1.

Algorithm:

temp=1, 1*2=2>1, stop.

ans.append(2^0 -1=0), ans=[0]

ans.append(1-0=1), ans=[0,1]

OR is 0|1=1 (1), one 1 bit.

Is there a better option?

[0,1]: OR 1, one 1 bit.

[1,0]: OR 1, one 1 bit.

No, it's the same.

Seems optimal.

Another example: n=1, k=1.

Simply print 1, which has one 1 bit.

Seems optimal.

So, overall, this algorithm seems to be achieving the maximum number of 1 bits in the OR for the given n and k.

Hence, the given program is correct.

**Final Answer**

\[ \boxed{\text{Correct}} \]