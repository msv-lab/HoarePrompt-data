To determine the correctness of the provided Python program, we need to verify whether it correctly computes the year in which the n-th sign occurs according to the problem statement.

Let's break down the logic of the program and compare it with the problem requirements:

1. **Initialization**: The program initializes `year` to 0. This variable keeps track of the current year as the signs occur sequentially.
2. **Loop through Signs**: For each sign (periodicity `ai`), the program calculates the next occurrence of the sign after the current `year`. This is done using the expression `year + (year % ai or ai)`.
   - `year % ai` gives the remainder when `year` is divided by `ai`, which tells us how far we are into the current cycle of the sign.
   - If `year % ai` is 0, it means the sign just occurred at `year`, so we need to wait for the next cycle, which is `ai` years later.
   - If `year % ai` is not 0, it means we need to wait until the next multiple of `ai` after `year`, which is `ai - (year % ai)` years later. However, since we are adding this to `year`, the expression simplifies to `year + ai - (year % ai)`, which is equivalent to `year + (year % ai or ai)` because `year % ai or ai` evaluates to `ai` when `year % ai` is 0.
3. **Update Year**: After calculating the next occurrence of the sign, the program updates `year` to this new value.
4. **Output**: After processing all signs for a test case, the program prints the final value of `year`, which represents the year in which the n-th sign occurs.

Given this explanation, the program appears to correctly simulate the sequential occurrence of the signs according to their periodicities and outputs the correct year for the n-th sign to occur.

Therefore, the correctness of the program is:

Correctness: **True**