Correctness: **False**

Explanation:
The provided program calculates the maximum sum and the sequence of operations to achieve that sum, but it does not correctly implement the operations as described in the problem statement. Specifically, the program does not correctly apply the permutations to the matrix, and the operations it prints do not ensure that the matrix is filled in a way that maximizes the sum.

Let's break down the issues:

1. **Sum Calculation**:
   - The program calculates the sum of the matrix as `sum = 1` and then adds `(i * i - (i - 1) * (i - 1)) * i` for `i` from 2 to `n`. This formula is incorrect for calculating the maximum sum of the matrix.
   - The maximum sum of the matrix can be achieved by filling each row with distinct integers from 1 to `n` in increasing order. The sum of the first `n` natural numbers is `n * (n + 1) / 2`. Since there are `n` rows, the total sum would be `n * (n * (n + 1) / 2) = n^2 * (n + 1) / 2`.

2. **Operations**:
   - The program prints `n + n` operations, which is correct in terms of the number of operations (no more than `2n` operations).
   - However, the operations themselves are not correctly described. The program prints the same permutation for each row and column, which does not ensure that each row and column contains distinct integers from 1 to `n`.

Here is a corrected version of the program:

```python
def func():
    t = int(input())
    for _ in range(t):
        n = int(input())
        max_sum = n * n * (n + 1) // 2
        print(max_sum, 2 * n)
        for i in range(1, n + 1):
            print(1, i, *range(1, n + 1))
            print(2, i, *range(1, n + 1))

```

In this corrected version:
- The maximum sum is calculated as `n^2 * (n + 1) / 2`.
- The operations are correctly described to fill each row and column with distinct integers from 1 to `n`.

Since the original program does not correctly implement the operations and the sum calculation, it is incorrect.