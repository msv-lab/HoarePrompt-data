Correctness: **False**

Explanation:
The provided solution does not correctly solve the problem as specified. The goal is to maximize the sum of the matrix elements, which can be achieved by filling the matrix with the maximum possible values (i.e., the integers from 1 to n in each row and column). The maximum possible sum for an \( n \times n \) matrix is \( n \times \frac{n(n + 1)}{2} \), which is the sum of all integers from 1 to n, repeated n times (once for each row or column).

However, the given program does not achieve this. Instead, it calculates a sum and a number of operations in a way that does not align with the problem's requirements. Specifically, the logic for calculating `sum` and `r` is flawed and does not correspond to the correct approach for maximizing the sum of the matrix.

A correct approach would be to perform \( n \) operations of type 1 and \( n \) operations of type 2, each using the permutation \( [1, 2, \ldots, n] \) to fill the matrix. This ensures that each element in the matrix is one of the integers from 1 to n, and the sum is maximized.

Here is a corrected version of the program:

```python
def func():
    t = int(input())
    for _ in range(t):
        n = int(input())
        max_sum = n * (n * (n + 1) // 2)
        operations = 2 * n
        print(max_sum, operations)
        for i in range(1, n + 1):
            print(1, i, *range(1, n + 1))
        for i in range(1, n + 1):
            print(2, i, *range(1, n + 1))

func()
```

This program correctly calculates the maximum possible sum and provides a valid sequence of operations to achieve it.