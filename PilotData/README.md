
<table>
  <tr>
    <td style="width: 20%; text-align: center;">
      <img src="../assets/HoarePrompt_logo.png" alt="HoarePrompt Logo" width="100"/>
    </td>
    <td style="width: 80%; text-align: left;">
      <h1>PilotData: Data generation for running HoarePrompt experiments</h1>
    </td>
  </tr>
</table>


## Introduction

The **PilotData** project is designed to generate, test, and evaluate code solutions for tasks from two benchmarks: the APPS benchmark and the MBPP Plus benchmark. Using large language models (LLMs) such as GPT-4.0 and LLaMA3, the project generates code and evaluates its correctness against canonical solutions. This data is then used to experiment with and test the HoarePrompt tool.

## Folder Structure

The **data** subfolder contains:

- **4 JSON files**: These represent the generated code by models GPT-4.0 and LLaMA3 for the APPS and MBPP datasets. They contain code generated for:
  - MBPP dataset using GPT-4.0
  - MBPP dataset using LLaMA3
  - APPS dataset using GPT-4.0
  - APPS dataset using LLaMA3
- **Pilot dataset JSONs**: Created by running `random_select.py`, these are subsets of the original 4 JSON files, containing a mix of tasks generated by the two models.
- **Mbppplus.jsonl**: The original MBPP dataset used as the input data.
- **APPS dataset**: **This is missing** but can be downloaded from [here](https://github.com/hendrycks/apps) due to its large size.

## Usage

### 1. Generate Code

- **Generate APPS benchmark data**:
  ```bash
  python3 apps_code_gen.py
  ```

- **Generate MBPP benchmark data**:
  ```bash
  python3 mbpp_code_gen.py
  ```

These commands will generate code for the respective datasets using GPT-4.0 and LLaMA models. The results, including accuracy and counterexamples, will be saved in JSON format in the `data` folder.

### 2. Random Selection for Pilot Data

- **Create Pilot Dataset Subsets**:
  ```bash
  python3 random_select.py --sample_size <sample_size>
  ```

The `random_select.py` script randomly selects a subset of tasks from the generated data for both APPS and MBPP. For each selected task, two versions (one from GPT-4.0 and one from LLaMA) are included. The final pilot dataset will contain `4 * sample_size` tasks and will be saved as `pilot_<number>_size_<sample_size>.json`.

### Few-Shot Examples

- `apps_code_gen.py` uses tasks `0000` and `2747` as few-shot examples for the APPS benchmark.
- `mbpp_code_gen.py` uses tasks `Mbpp/2`, `Mbpp/3`, and `Mbpp/4` as few-shot examples for the MBPP Plus benchmark.

### Testing

- For **MBPP**, correctness is evaluated by executing the `canonical_solution` with the provided test inputs using Python's `exec()` function.
- For **APPS**, outputs are captured using the `subprocess` module to handle the standard output.

**Note**: MBPP task `255` is skipped due to the potential complexity of the generated answers.

## Scripts Overview

### `mbpp_code_gen.py`

This script generates Python code for tasks from the MBPP Plus dataset. It prompts a model (the model can be selected by commenting or uncommenting in the source code) using a series of prompts and evaluates the generated code using predefined tests. It saves the results in JSON format, including base accuracy, additional accuracy, and assertion results. The script tries multiple times to generate syntactically correct code.

- Optional argument: `--save` to specify the save directory; otherwise, it defaults to `./data`.

### `mbppplus_test.py`

This script tests the generated Python functions for MBPP tasks by running them against predefined inputs. It compares the outputs to expected values and calculates accuracy metrics such as base accuracy and assertion accuracy. It also ensures that generated code passes assertion tests and handles potential execution timeouts.

### `apps_code_gen.py`

This script generates code for APPS tasks using an LLM. It retrieves problem descriptions, inputs, and outputs, and generates solutions based on the descriptions. The generated code is then validated against the test cases, and results are saved in JSON format.

**Note**: The APPS dataset is missing but can be downloaded from [here](https://github.com/hendrycks/apps).

### `apps_test.py`

This script is responsible for testing the generated code for APPS tasks. It runs the generated code with specific inputs and compares the output with expected results. The script uses Pythonâ€™s `subprocess` module to execute the code in isolation and handle errors such as timeouts.

### `random_select.py`

This script randomly selects matched tasks from two datasets (APPS and MBPP) generated by GPT-4.0 and LLaMA3 models. It ensures that the same task exists in both datasets and randomly selects a specified number of tasks where both models generated code for the same task.

**Criteria for Selection**:
- The task must be present in both datasets.
- The correctness of the task is determined by its pass rates.
- Counterexample information is gathered if available.

- Optional argument: `--sample_size` to control the number of selected examples.

### `model.py`

This script provides an interface to select the appropriate LLM (OpenAI or Groq) based on the model name. It includes retry logic to handle failures when querying the models. The `get_model()` function returns the appropriate model object for querying either OpenAI or Groq.

## Data Subdirectory Contents

- **APPS and MBPP Code**: The data generated for APPS and MBPP benchmarks by GPT-4.0 and LLaMA3 models. These JSON files are used to generate subsets for experimentation.
- **Pilot Datasets**: Subsets of tasks from the 4 original JSON files, created using `random_select.py`. These datasets are used for initial testing and experimentation with HoarePrompt.
- **Mbppplus.jsonl**: The original dataset for MBPP Plus tasks.

## Next Steps

To start using the generated datasets for testing and experimentation with **HoarePrompt**, you can run the scripts in the **PilotData** folder to generate new code solutions or to create pilot datasets for experiments based on the already generated solutions. 
You can use any of the Pilot jsons or create on of your desired size and use it as the Data File for the HoarePrompt tool using the capabilities of the [HoarePrompt-experiments repo](https://github.com/msv-lab/HoarePrompt-experiments)

For moreinformation  regarding the missing APPS datasetand to  download it visit the following [link](https://github.com/hendrycks/apps).

## Contributions
This is a joint project collaboration of Peking Univeristy and University College London.
Feel free to contribute to HoarePrompt-data by opening issues or submitting pull requests on GitHub. Your contributions are highly appreciated!

<table>
  <tr>
    <td style="text-align: center;">
      <img src="../assets/PKU.png" alt="Image 1" width="300"/>
    </td>
    <td style="text-align: center;">
      <img src="../assets/HoarePrompt_logo.png" alt="Image 2" width="300"/>
    </td>
    <td style="text-align: center;">
      <img src="../assets/UCL.png" alt="Image 3" width="300"/>
    </td>
  </tr>
</table>